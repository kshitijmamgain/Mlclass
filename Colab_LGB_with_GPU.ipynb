{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LGB_GPU.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1pMPktNMdWnXQ4Ag4PBisQ6joROY7L1pf",
      "authorship_tag": "ABX9TyNVYCYf8vm3EOUE8bspYQ8S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kshitijmamgain/Mlclass/blob/master/Colab_LGB_with_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xaHJgxY_DKW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "35e91cee-355e-4be2-8f7b-e60168797f3a"
      },
      "source": [
        "!bash colablgb.sh"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "colablgb.sh: line 1: !git: command not found\n",
            "colablgb.sh: line 2: $'\\r': command not found\n",
            "colablgb.sh: line 3: !cd: command not found\n",
            "colablgb.sh: line 4: $'\\r': command not found\n",
            "colablgb.sh: line 5: !mkdir: command not found\n",
            "colablgb.sh: line 6: $'\\r': command not found\n",
            "colablgb.sh: line 7: !cmake: command not found\n",
            "colablgb.sh: line 8: $'\\r': command not found\n",
            "colablgb.sh: line 9: !make: command not found\n",
            "colablgb.sh: line 10: $'\\r': command not found\n",
            "colablgb.sh: line 11: !sudo: command not found\n",
            "colablgb.sh: line 12: $'\\r': command not found\n",
            "colablgb.sh: line 13: !sudo: command not found\n",
            "colablgb.sh: line 14: $'\\r': command not found\n",
            "colablgb.sh: line 15: !cd: command not found\n",
            "colablgb.sh: line 16: $'\\r': command not found\n",
            "colablgb.sh: line 17: !sudo: command not found\n",
            "colablgb.sh: line 18: $'\\r': command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl_GTKqum-ts",
        "colab_type": "code",
        "outputId": "e38531fe-1f85-45a1-ff8d-61db6077de87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone -- https://github.com/Microsoft/LightGBM"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'LightGBM' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0_psjp7nJFe",
        "colab_type": "code",
        "outputId": "32287eef-7dc3-423b-ef27-ef72ee4aeb14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA1TQrP4nbpN",
        "colab_type": "code",
        "outputId": "eb60aa92-40ab-46a4-c92e-75edc3525320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/LightGBM/"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/LightGBM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYU94ye7nio-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d9bf40f-dbdc-4f20-9b21-9fdbba642510"
      },
      "source": [
        "!mkdir build"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘build’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WySFRPEpnlYs",
        "colab_type": "code",
        "outputId": "18220c88-cd34-4246-b057-49ed1fc1ffe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!cmake -DUSE_GPU=1 #avoid .."
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Found OpenMP_C: -fopenmp  \n",
            "-- Found OpenMP_CXX: -fopenmp  \n",
            "-- Found OpenMP: TRUE   \n",
            "-- OpenCL include directory: /usr/include\n",
            "-- Boost version: 1.65.1\n",
            "-- Found the following Boost libraries:\n",
            "--   filesystem\n",
            "--   system\n",
            "-- Using _mm_prefetch\n",
            "-- Using _mm_malloc\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/LightGBM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45R55t9ensF6",
        "colab_type": "code",
        "outputId": "50307739-b957-43d9-9a22-c5594a29725d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!make -j$(nproc)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[35m\u001b[1mScanning dependencies of target lightgbm\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target _lightgbm\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/main.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/application/application.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/application/application.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/boosting.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/boosting.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/gbdt.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/gbdt.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/gbdt_model_text.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/gbdt_model_text.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/gbdt_prediction.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/gbdt_prediction.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/prediction_early_stop.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/bin.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/prediction_early_stop.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/bin.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/config.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/config.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/config_auto.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/config_auto.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/dataset.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/dataset.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/dataset_loader.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/dataset_loader.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/file_io.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/json11.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/file_io.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/json11.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/metadata.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/metadata.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/parser.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/parser.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/tree.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/tree.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/metric/dcg_calculator.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/metric/dcg_calculator.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/metric/metric.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/metric/metric.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/network/linker_topo.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/network/linkers_mpi.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/linker_topo.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/network/linkers_socket.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/linkers_mpi.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/linkers_socket.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/network/network.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/network.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/objective/objective_function.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/objective/objective_function.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/data_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/data_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/feature_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/feature_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/gpu_tree_learner.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/gpu_tree_learner.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/serial_tree_learner.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/serial_tree_learner.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/tree_learner.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/tree_learner.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/voting_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/voting_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable lightgbm\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/c_api.cpp.o\u001b[0m\n",
            "[ 98%] Built target lightgbm\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX shared library lib_lightgbm.so\u001b[0m\n",
            "[100%] Built target _lightgbm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-vif0G3n9Vj",
        "colab_type": "code",
        "outputId": "bf75069e-3c89-43c3-8bb7-49ca2065202a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!sudo apt-get -y install python-pip"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython-all-dev python-all python-all-dev python-asn1crypto\n",
            "  python-cffi-backend python-crypto python-cryptography python-dbus\n",
            "  python-enum34 python-gi python-idna python-ipaddress python-keyring\n",
            "  python-keyrings.alt python-pip-whl python-pkg-resources python-secretstorage\n",
            "  python-setuptools python-six python-wheel python-xdg\n",
            "Suggested packages:\n",
            "  python-crypto-doc python-cryptography-doc python-cryptography-vectors\n",
            "  python-dbus-dbg python-dbus-doc python-enum34-doc python-gi-cairo\n",
            "  gnome-keyring libkf5wallet-bin gir1.2-gnomekeyring-1.0 python-fs\n",
            "  python-gdata python-keyczar python-secretstorage-doc python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  libpython-all-dev python-all python-all-dev python-asn1crypto\n",
            "  python-cffi-backend python-crypto python-cryptography python-dbus\n",
            "  python-enum34 python-gi python-idna python-ipaddress python-keyring\n",
            "  python-keyrings.alt python-pip python-pip-whl python-pkg-resources\n",
            "  python-secretstorage python-setuptools python-six python-wheel python-xdg\n",
            "0 upgraded, 22 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 3,376 kB of archives.\n",
            "After this operation, 10.5 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpython-all-dev amd64 2.7.15~rc1-1 [1,092 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-all amd64 2.7.15~rc1-1 [1,076 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-all-dev amd64 2.7.15~rc1-1 [1,100 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-asn1crypto all 0.24.0-1 [72.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-cffi-backend amd64 1.11.5-1 [63.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-crypto amd64 2.6.1-8ubuntu2 [244 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-enum34 all 1.1.6-2 [34.8 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-idna all 2.6-1 [32.4 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-ipaddress all 1.0.17-1 [18.2 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-six all 1.11.0-2 [11.3 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python-cryptography amd64 2.1.4-1ubuntu1.3 [221 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-dbus amd64 1.2.6-1 [90.2 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python-gi amd64 3.26.1-2ubuntu1 [197 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-secretstorage all 2.3.1-2 [11.8 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-keyring all 10.6.0-1 [30.6 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-keyrings.alt all 3.0-1 [16.7 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python-pip-whl all 9.0.1-2.3~ubuntu1.18.04.1 [1,653 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python-pip all 9.0.1-2.3~ubuntu1.18.04.1 [151 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-pkg-resources all 39.0.1-2 [128 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-setuptools all 39.0.1-2 [329 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-wheel all 0.30.0-0.2 [36.4 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-xdg all 0.25-4ubuntu1 [31.3 kB]\n",
            "Fetched 3,376 kB in 3s (1,133 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 22.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython-all-dev:amd64.\n",
            "(Reading database ... 144433 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libpython-all-dev_2.7.15~rc1-1_amd64.deb ...\n",
            "Unpacking libpython-all-dev:amd64 (2.7.15~rc1-1) ...\n",
            "Selecting previously unselected package python-all.\n",
            "Preparing to unpack .../01-python-all_2.7.15~rc1-1_amd64.deb ...\n",
            "Unpacking python-all (2.7.15~rc1-1) ...\n",
            "Selecting previously unselected package python-all-dev.\n",
            "Preparing to unpack .../02-python-all-dev_2.7.15~rc1-1_amd64.deb ...\n",
            "Unpacking python-all-dev (2.7.15~rc1-1) ...\n",
            "Selecting previously unselected package python-asn1crypto.\n",
            "Preparing to unpack .../03-python-asn1crypto_0.24.0-1_all.deb ...\n",
            "Unpacking python-asn1crypto (0.24.0-1) ...\n",
            "Selecting previously unselected package python-cffi-backend.\n",
            "Preparing to unpack .../04-python-cffi-backend_1.11.5-1_amd64.deb ...\n",
            "Unpacking python-cffi-backend (1.11.5-1) ...\n",
            "Selecting previously unselected package python-crypto.\n",
            "Preparing to unpack .../05-python-crypto_2.6.1-8ubuntu2_amd64.deb ...\n",
            "Unpacking python-crypto (2.6.1-8ubuntu2) ...\n",
            "Selecting previously unselected package python-enum34.\n",
            "Preparing to unpack .../06-python-enum34_1.1.6-2_all.deb ...\n",
            "Unpacking python-enum34 (1.1.6-2) ...\n",
            "Selecting previously unselected package python-idna.\n",
            "Preparing to unpack .../07-python-idna_2.6-1_all.deb ...\n",
            "Unpacking python-idna (2.6-1) ...\n",
            "Selecting previously unselected package python-ipaddress.\n",
            "Preparing to unpack .../08-python-ipaddress_1.0.17-1_all.deb ...\n",
            "Unpacking python-ipaddress (1.0.17-1) ...\n",
            "Selecting previously unselected package python-six.\n",
            "Preparing to unpack .../09-python-six_1.11.0-2_all.deb ...\n",
            "Unpacking python-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python-cryptography.\n",
            "Preparing to unpack .../10-python-cryptography_2.1.4-1ubuntu1.3_amd64.deb ...\n",
            "Unpacking python-cryptography (2.1.4-1ubuntu1.3) ...\n",
            "Selecting previously unselected package python-dbus.\n",
            "Preparing to unpack .../11-python-dbus_1.2.6-1_amd64.deb ...\n",
            "Unpacking python-dbus (1.2.6-1) ...\n",
            "Selecting previously unselected package python-gi.\n",
            "Preparing to unpack .../12-python-gi_3.26.1-2ubuntu1_amd64.deb ...\n",
            "Unpacking python-gi (3.26.1-2ubuntu1) ...\n",
            "Selecting previously unselected package python-secretstorage.\n",
            "Preparing to unpack .../13-python-secretstorage_2.3.1-2_all.deb ...\n",
            "Unpacking python-secretstorage (2.3.1-2) ...\n",
            "Selecting previously unselected package python-keyring.\n",
            "Preparing to unpack .../14-python-keyring_10.6.0-1_all.deb ...\n",
            "Unpacking python-keyring (10.6.0-1) ...\n",
            "Selecting previously unselected package python-keyrings.alt.\n",
            "Preparing to unpack .../15-python-keyrings.alt_3.0-1_all.deb ...\n",
            "Unpacking python-keyrings.alt (3.0-1) ...\n",
            "Selecting previously unselected package python-pip-whl.\n",
            "Preparing to unpack .../16-python-pip-whl_9.0.1-2.3~ubuntu1.18.04.1_all.deb ...\n",
            "Unpacking python-pip-whl (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
            "Selecting previously unselected package python-pip.\n",
            "Preparing to unpack .../17-python-pip_9.0.1-2.3~ubuntu1.18.04.1_all.deb ...\n",
            "Unpacking python-pip (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
            "Selecting previously unselected package python-pkg-resources.\n",
            "Preparing to unpack .../18-python-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python-setuptools.\n",
            "Preparing to unpack .../19-python-setuptools_39.0.1-2_all.deb ...\n",
            "Unpacking python-setuptools (39.0.1-2) ...\n",
            "Selecting previously unselected package python-wheel.\n",
            "Preparing to unpack .../20-python-wheel_0.30.0-0.2_all.deb ...\n",
            "Unpacking python-wheel (0.30.0-0.2) ...\n",
            "Selecting previously unselected package python-xdg.\n",
            "Preparing to unpack .../21-python-xdg_0.25-4ubuntu1_all.deb ...\n",
            "Unpacking python-xdg (0.25-4ubuntu1) ...\n",
            "Setting up python-idna (2.6-1) ...\n",
            "Setting up python-pip-whl (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
            "Setting up python-asn1crypto (0.24.0-1) ...\n",
            "Setting up python-crypto (2.6.1-8ubuntu2) ...\n",
            "Setting up python-wheel (0.30.0-0.2) ...\n",
            "Setting up libpython-all-dev:amd64 (2.7.15~rc1-1) ...\n",
            "Setting up python-pkg-resources (39.0.1-2) ...\n",
            "Setting up python-cffi-backend (1.11.5-1) ...\n",
            "Setting up python-gi (3.26.1-2ubuntu1) ...\n",
            "Setting up python-six (1.11.0-2) ...\n",
            "Setting up python-enum34 (1.1.6-2) ...\n",
            "Setting up python-dbus (1.2.6-1) ...\n",
            "Setting up python-ipaddress (1.0.17-1) ...\n",
            "Setting up python-pip (9.0.1-2.3~ubuntu1.18.04.1) ...\n",
            "Setting up python-all (2.7.15~rc1-1) ...\n",
            "Setting up python-xdg (0.25-4ubuntu1) ...\n",
            "Setting up python-setuptools (39.0.1-2) ...\n",
            "Setting up python-keyrings.alt (3.0-1) ...\n",
            "Setting up python-all-dev (2.7.15~rc1-1) ...\n",
            "Setting up python-cryptography (2.1.4-1ubuntu1.3) ...\n",
            "Setting up python-secretstorage (2.3.1-2) ...\n",
            "Setting up python-keyring (10.6.0-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n86ENWdpIfD",
        "colab_type": "code",
        "outputId": "daa1796a-b8de-4653-b120-6ab76a8fc58f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!sudo -H pip install setuptools pandas numpy scipy scikit-learn -U"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting setuptools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/c5/0d38afb961f83e0d51f319f7dc166195ebabc1ea3cb20a10a77f500f7156/setuptools-46.4.0-py3-none-any.whl (583kB)\n",
            "\u001b[K     |████████████████████████████████| 583kB 2.8MB/s \n",
            "\u001b[?25hRequirement already up-to-date: pandas in /usr/local/lib/python3.6/dist-packages (1.0.3)\n",
            "Requirement already up-to-date: numpy in /usr/local/lib/python3.6/dist-packages (1.18.4)\n",
            "Requirement already up-to-date: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/3a/eb8d7bbe28f4787d140bb9df685b7d5bf6115c0e2a969def4027144e98b6/scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 13.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.15.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/db/09/cab2f398e28e9f183714afde872b2ce23629f5833e467b151f18e1e08908/threadpoolctl-2.0.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: setuptools, threadpoolctl, scikit-learn\n",
            "  Found existing installation: setuptools 46.3.0\n",
            "    Uninstalling setuptools-46.3.0:\n",
            "      Successfully uninstalled setuptools-46.3.0\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.23.1 setuptools-46.4.0 threadpoolctl-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI3iMOzYpYLY",
        "colab_type": "code",
        "outputId": "f381286a-1abf-4913-c1dc-2f141b775d9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/LightGBM/python-package"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/LightGBM/python-package\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH68FES3CXig",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "7ebeaeba-ece5-4bd3-a711-5b445a6f3e25"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat May 23 19:36:52 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kph88UQBpkoZ",
        "colab_type": "code",
        "outputId": "958f68e7-e37c-42be-b3ed-952ff58d3390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!sudo python setup.py install --precompile"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running install\n",
            "running build\n",
            "running build_py\n",
            "INFO:root:Generating grammar tables from /usr/lib/python3.6/lib2to3/Grammar.txt\n",
            "INFO:root:Generating grammar tables from /usr/lib/python3.6/lib2to3/PatternGrammar.txt\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/lightgbm\n",
            "copying lightgbm/compat.py -> build/lib/lightgbm\n",
            "copying lightgbm/__init__.py -> build/lib/lightgbm\n",
            "copying lightgbm/libpath.py -> build/lib/lightgbm\n",
            "copying lightgbm/callback.py -> build/lib/lightgbm\n",
            "copying lightgbm/engine.py -> build/lib/lightgbm\n",
            "copying lightgbm/sklearn.py -> build/lib/lightgbm\n",
            "copying lightgbm/plotting.py -> build/lib/lightgbm\n",
            "copying lightgbm/basic.py -> build/lib/lightgbm\n",
            "running egg_info\n",
            "creating lightgbm.egg-info\n",
            "writing lightgbm.egg-info/PKG-INFO\n",
            "writing dependency_links to lightgbm.egg-info/dependency_links.txt\n",
            "writing requirements to lightgbm.egg-info/requires.txt\n",
            "writing top-level names to lightgbm.egg-info/top_level.txt\n",
            "writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "no previously-included directories found matching 'build'\n",
            "warning: no files found matching 'LICENSE'\n",
            "warning: no files found matching '*.txt'\n",
            "warning: no files found matching '*.so' under directory 'lightgbm'\n",
            "warning: no files found matching '*.txt' under directory 'compile'\n",
            "warning: no files found matching '*.so' under directory 'compile'\n",
            "warning: no files found matching '*.dll' under directory 'compile/Release'\n",
            "warning: no files found matching '*' under directory 'compile/compute'\n",
            "warning: no files found matching '*' under directory 'compile/include'\n",
            "warning: no files found matching '*' under directory 'compile/src'\n",
            "warning: no files found matching 'LightGBM.sln' under directory 'compile/windows'\n",
            "warning: no files found matching 'LightGBM.vcxproj' under directory 'compile/windows'\n",
            "warning: no files found matching '*.dll' under directory 'compile/windows/x64/DLL'\n",
            "warning: no previously-included files matching '*.py[co]' found anywhere in distribution\n",
            "writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
            "copying lightgbm/VERSION.txt -> build/lib/lightgbm\n",
            "running install_lib\n",
            "copying build/lib/lightgbm/compat.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/__init__.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/libpath.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/callback.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/engine.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/sklearn.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/plotting.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/VERSION.txt -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/basic.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "INFO:LightGBM:Installing lib_lightgbm from: ['../lib_lightgbm.so']\n",
            "copying ../lib_lightgbm.so -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/compat.py to compat.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/libpath.py to libpath.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/callback.py to callback.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/engine.py to engine.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/sklearn.py to sklearn.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/plotting.py to plotting.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/basic.py to basic.cpython-36.pyc\n",
            "running install_egg_info\n",
            "Copying lightgbm.egg-info to /usr/local/lib/python3.6/dist-packages/lightgbm-2.3.2-py3.6.egg-info\n",
            "running install_scripts\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQwRBkazpnG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_csv('/content/sample_data/california_housing_train.csv', sep=',')\n",
        "df_test = pd.read_csv('/content/sample_data/california_housing_test.csv', sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-sO0_jPrhoq",
        "colab_type": "code",
        "outputId": "20763cff-4f4a-4c7b-ba76-094ce57aadda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "df_train.columns"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
              "       'total_bedrooms', 'population', 'households', 'median_income',\n",
              "       'median_house_value'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhnu8tQMr5jH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anm9nvOksR3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = df_train['median_house_value']\n",
        "y_test = df_test['median_house_value']\n",
        "X_train = df_train.drop('median_house_value', axis=1)\n",
        "X_test = df_test.drop('median_house_value', axis=1)\n",
        "\n",
        "# create dataset for lightgbm\n",
        "lgb_train = lgb.Dataset(X_train, y_train)\n",
        "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIIraMWUsZv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specify your configurations as a dict\n",
        "params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'regression',\n",
        "    'metric': {'l2', 'l1'},\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': 0,\n",
        "    'device': 'gpu'\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64jBJNUlsgrX",
        "colab_type": "code",
        "outputId": "baa7be31-d75c-49ac-fd44-6bf67d376489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "print('Starting training...')\n",
        "# train\n",
        "gbm = lgb.train(params,\n",
        "                lgb_train,\n",
        "                num_boost_round=20,\n",
        "                valid_sets=lgb_eval,\n",
        "                early_stopping_rounds=5)\n",
        "\n",
        "print('Saving model...')\n",
        "# save model to file\n",
        "gbm.save_model('model.txt')\n",
        "\n",
        "print('Starting predicting...')\n",
        "# predict\n",
        "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
        "# eval\n",
        "print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "LightGBMError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-47b84abb492f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlgb_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 early_stopping_rounds=5)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saving model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mreduced_valid_sets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalid_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0mname_valid_sets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mname_valid_sets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valid_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, train_set, model_file, silent)\u001b[0m\n\u001b[1;32m   1552\u001b[0m             \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_to_1d_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'group'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'group'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_safe_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \"\"\"Check the return value from C API call.\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLightGBMError\u001b[0m: GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIQOdUsJsks_",
        "colab_type": "code",
        "outputId": "66ae4f8e-387d-4c3f-9158-3f9f90aecb74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install optuna"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/7a/b6e3ddae75af5f7d987b4fc6be7bb38d9c9c195bd662d8762a75163c1103/optuna-1.4.0.tar.gz (183kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 2.7MB/s \n",
            "\u001b[?25hCollecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/1e/cabc75a189de0fbb2841d0975243e59bde8b7822bacbb95008ac6fe9ad47/alembic-1.4.2.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 10.6MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cliff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/17/57187872842bf9f65815b6969b515528ec7fd754137d2d3f49e3bc016175/cliff-3.1.0-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.5MB/s \n",
            "\u001b[?25hCollecting cmaes>=0.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/03/de/6ed34ebc0e5c34ed371d898540bca36edb4afe5bb2ca382483054e573c75/cmaes-0.5.0-py3-none-any.whl\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/00/0d/22c73c2eccb21dd3498df7d22c0b1d4a30f5a5fb3feb64e1ce06bc247747/colorlog-4.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (0.15.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.18.4)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.17)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/78/f6ade1e18aebda570eed33b7c534378d9659351cadce2fcbc7b31be5f615/Mako-1.1.2-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.3MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (2.8.1)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (3.13)\n",
            "Requirement already satisfied: PrettyTable<0.8,>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (0.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.4.7)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.12.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/ba/aa953a11ec014b23df057ecdbc922fdb40ca8463466b1193f3367d2711a6/pbr-5.4.5-py2.py3-none-any.whl (110kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 17.2MB/s \n",
            "\u001b[?25hCollecting stevedore>=1.20.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/49/a35dd566626892d577e426dbe5ea424dd7fbe10645f2c1070dcba474eca9/stevedore-1.32.0-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.8MB/s \n",
            "\u001b[?25hCollecting cmd2!=0.8.3,<0.9.0,>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/40/a71caa2aaff10c73612a7106e2d35f693e85b8cf6e37ab0774274bca3cf9/cmd2-0.8.9-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n",
            "Requirement already satisfied: wcwidth; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,<0.9.0,>=0.8.0->cliff->optuna) (0.1.9)\n",
            "Collecting pyperclip\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/5b/55866e1cde0f86f5eec59dab5de8a66628cb0d53da74b8dbc15ad8dabda3/pyperclip-1.8.0.tar.gz\n",
            "Building wheels for collected packages: alembic\n",
            "  Building wheel for alembic (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.2-cp36-none-any.whl size=159543 sha256=5414ddea60d339752c69f68a9cbd086de5352f5536bac2640ca661efd1e90dce\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/04/83/76023f7a4c14688c0b5c2682a96392cfdd3ee4449eaaa287ef\n",
            "Successfully built alembic\n",
            "Building wheels for collected packages: optuna, pyperclip\n",
            "  Building wheel for optuna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-1.4.0-cp36-none-any.whl size=254554 sha256=9989a6cddcd72b72a4c2e84952d12541457b057a259f0df1334603641502b1e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/48/95/5257941c80e08f78a7afbd7981f24f9a69a83a5a1c3f160ff1\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.0-cp36-none-any.whl size=8693 sha256=0327f651355db0dce7ef6d2dd651ee865c8ddd0ec744d879e1f1563cfdb890a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/ac/0a/b784f0afe26eaf52e88a7e15c7369090deea0354fa1c6fc689\n",
            "Successfully built optuna pyperclip\n",
            "Installing collected packages: Mako, python-editor, alembic, pbr, stevedore, pyperclip, cmd2, cliff, cmaes, colorlog, optuna\n",
            "Successfully installed Mako-1.1.2 alembic-1.4.2 cliff-3.1.0 cmaes-0.5.0 cmd2-0.8.9 colorlog-4.1.0 optuna-1.4.0 pbr-5.4.5 pyperclip-1.8.0 python-editor-1.0.4 stevedore-1.32.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORSmYLxptlqM",
        "colab_type": "code",
        "outputId": "01809760-824d-41bd-b16e-032e5744c4a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "\n",
        "# Draft Codes\n",
        "# coding: utf-8\n",
        "''' This class tunes hyperparamter for LightGBM ML algorithm for Higgs dataset'''\n",
        "import ast \n",
        "import csv\n",
        "from timeit import default_timer as timer\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import auc, accuracy_score, roc_auc_score, roc_curve, confusion_matrix, precision_recall_curve\n",
        "from hyperopt import STATUS_OK, STATUS_FAIL, hp, tpe, Trials, fmin\n",
        "import optuna.integration.lightgbm as lgbo\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# defining constant\n",
        "MAX_EVALS = 100\n",
        "N_FOLDS = 3\n",
        "NUM_BOOST_ROUNDS = 10000\n",
        "EARLY_STOPPING_ROUNDS = 100\n",
        "SEED = 47\n",
        "RESULT_PATH = 'lgbm.csv'\n",
        "FILE_PATH = \"/content/drive/My Drive/Colab Notebooks/train_test_files_sample.csv\"\n",
        "OBJECTIVE_LOSS = 'binary' # use cross_entropy\n",
        "EVAL_METRIC = ['auc', 'binary', 'xentropy']\n",
        "#   for google colab\n",
        "\n",
        "# starting with storing the data as data frame\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "df.drop(columns='Unnamed: 0', inplace=True)\n",
        "\n",
        "# making a smaller df for quick testing\n",
        "df_s, _ = train_test_split(df, random_state=30, train_size=0.1)\n",
        "train_X = df_s.drop(columns='0')\n",
        "train_y = df['0']\n",
        "\n",
        "# drop last columns\n",
        "def col_keep(df):\n",
        "    '''Removes the last 7 columns'''\n",
        "    return df.drop(columns=list(map(str, range(22, 29))), inplace=True) # removing 7 last columns\n",
        "col_keep(train_X)\n",
        "\n",
        "# random search\n",
        "PARAM_GRID = {\n",
        "    'num_leaves': list(range(16, 196, 4)),\n",
        "    'max_bin': [254],\n",
        "    'lambda_l1': list(np.linspace(0, 1)),\n",
        "    'lambda_l2': list(np.linspace(0, 1)),\n",
        "    'min_data_in_leaf' : list(range(20, 500, 10)),\n",
        "    'boosting_type': ['gbdt', 'goss'],\n",
        "    'learning_rate' : list(np.logspace(np.log(0.05), np.log(0.2), base=np.exp(1), num=1000)),\n",
        "    'feature_fraction': list(np.linspace(0.4, 1.0)),\n",
        "    'subsample_for_bin': list(range(20000, 300000, 20000)),\n",
        "    'bagging_freq': list(range(1, 7)),\n",
        "    'verbosity' : [0],\n",
        "    'objective' : [OBJECTIVE_LOSS]\n",
        "    }\n",
        "\n",
        "\n",
        "# Hyperopt Space\n",
        "H_SPACE = {\n",
        "    'num_leaves': hp.quniform('num_leaves', 16, 196, 4),\n",
        "\n",
        "    'lambda_l1': hp.uniform('lambda_l1', 0.0, 1.0),\n",
        "    'lambda_l2': hp.uniform(\"lambda_l2\", 0.0, 1.0),\n",
        "    'boosting_type': hp.choice('boosting_type',\n",
        "                               [{'boosting_type': 'gbdt',\n",
        "                                 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)},\n",
        "                                #{'boosting_type': 'dart',\n",
        "                                #'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n",
        "                                {'boosting_type': 'goss', 'subsample': 1.0}]),\n",
        "    'learning_rate' : hp.loguniform('learning_rate', np.log(0.05), np.log(0.25)),\n",
        "    'feature_fraction': hp.uniform('feature_fraction', 0.4, 1.0),\n",
        "    'bagging_freq': hp.uniform('bagging_freq', 1, 7),\n",
        "    'verbosity' : 0,\n",
        "    'objective' : OBJECTIVE_LOSS,\n",
        "    'device' :'gpu'\n",
        "    }\n",
        "\n",
        "class Lgbmclass():\n",
        "    '''Parameter Tuning Class tunes the LightGBM model with different optimization techniques -\n",
        "    Hyperopt, Optuna and RandomSearch.'''\n",
        "    iteration = 0\n",
        "    def __init__(self, x_train, y_train):\n",
        "        '''Initializes the Parameter tuning class and also initializes LightGBM dataset object\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_train: data (string, numpy array, pandas DataFrame, H2O DataTable's Frame, scipy.sparse\n",
        "        or list of numpy arrays) – Data source of Dataset. If string, it represents the path to\n",
        "        txt file.\n",
        "        y_train: label (list, numpy 1-D array, pandas Series / one-column DataFrame or None,\n",
        "        optional (default=None)) – Label of the data.'''\n",
        "\n",
        "        \n",
        "\n",
        "        # File to save first results\n",
        "        self.out_file = RESULT_PATH\n",
        "        with open(self.out_file, 'w', newline='') as of_connection:\n",
        "            writer = csv.writer(of_connection)\n",
        "            # Write the headers to the file\n",
        "            writer.writerow(['loss', 'params', 'iteration', 'estimators', 'train_time','optim_type'])\n",
        "\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.train_set = lgb.Dataset(data=train_X, label=train_y)\n",
        "\n",
        "    def parameter_tuning(self, tune_type, diagnostic=False):\n",
        "        '''\n",
        "        '''\n",
        "        methodlist = ['hyperopt_space','optuna_space','random_space']\n",
        "        optim_type = 'random_space'\n",
        "        if optim_type not in methodlist:\n",
        "            raise TypeError('Otimization type must have a valid space:',\n",
        "                            '\\n\\t\\t hyperopt_space, optuna_space or random_space')\n",
        "        tuner = getattr(self, tune_type)\n",
        "        if diagnostic:\n",
        "            return(tuner())\n",
        "        else:\n",
        "            tuner()\n",
        "            return self.params\n",
        "\n",
        "    def lgb_crossval(self, params, optim_type):\n",
        "        '''lgb cross validation model\n",
        "        Paramters\n",
        "        ---------\n",
        "        params: Hyper parameters in dict type from different optimization methods\n",
        "        optim_type: Is the type of optimization called we use lgb integration for optuna type\n",
        "        Returns\n",
        "        ------\n",
        "        Loss, params, n_estimator, run_time'''\n",
        "        # initializing the timer\n",
        "         \n",
        "        start = timer()\n",
        "        if optim_type == 'optuna':\n",
        "            cv_results = lgbo.cv(params, self.train_set, num_boost_round=NUM_BOOST_ROUNDS,\n",
        "                                 nfold=N_FOLDS, early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
        "                                 metrics=EVAL_METRIC, seed=SEED)\n",
        "        else:\n",
        "            cv_results = lgb.cv(params, self.train_set, num_boost_round=NUM_BOOST_ROUNDS,\n",
        "                                nfold=N_FOLDS, early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
        "                                metrics=EVAL_METRIC, seed=SEED)\n",
        "        # store the runtime\n",
        "        run_time = timer() - start\n",
        "\n",
        "        # Extract the best score\n",
        "        best_score = np.max(cv_results['auc-mean'])\n",
        "\n",
        "        # Loss must be minimized\n",
        "        loss = 1 - best_score\n",
        "\n",
        "        # Boosting rounds that returned the highest cv score\n",
        "        n_estimators = int(np.argmax(cv_results['auc-mean']) + 1) # more explanation\n",
        "        self.estimator = n_estimators\n",
        "\n",
        "        # Write to the csv file ('a' means append)\n",
        "        of_connection = open(self.out_file, 'a')\n",
        "        writer = csv.writer(of_connection)\n",
        "        writer.writerow([loss, params, self.iteration, n_estimators, run_time, optim_type])\n",
        "\n",
        "        return loss, params, n_estimators, run_time\n",
        "\n",
        "    def hyperopt_space(self):\n",
        "        '''An Lgbm class method to call the hyperopt optimization for the data\n",
        "        Parameters\n",
        "        ----------\n",
        "        fn_name: is the objective function to minimize defined with in the class function\n",
        "        space: is dictionary type hypeorpt space over which the search is done\n",
        "        algo: is the type of search algorithm\n",
        "        trials: Hyperopt base trials object\n",
        "        Returns\n",
        "        -------\n",
        "        result: best parameter that minimizes the fn_name over max_evals = MAX_EVALS FIXED FOR TESTING\n",
        "        trials: the database in which to store all the point evaluations of the search'''\n",
        "        print('Running {} rounds of LGBM parameter optimisation using Hyperopt:'.format(MAX_EVALS))\n",
        "        fn_name, space, algo, trials='hyperopt_obj', H_SPACE, tpe.suggest, Trials()\n",
        "        fn = getattr(self, fn_name)\n",
        "        try:\n",
        "            result = fmin(fn=fn, space=space, algo=algo, max_evals=MAX_EVALS,\n",
        "                          trials=trials, rstate=np.random.RandomState(SEED))\n",
        "        except Exception as e:\n",
        "            return {'status': STATUS_FAIL, 'exception': str(e)}\n",
        "        self.params = trials.best_trial['result']['params']\n",
        "        self.params['n_estimator'] = self.estimator\n",
        "        return result, trials\n",
        "\n",
        "    def optuna_space(self):\n",
        "        '''An Optuna class method to call the Optuna optimization for the data\n",
        "        Parameters\n",
        "        ----------\n",
        "        fn_name: is the optuna objective function to minimize defined with in the class function\n",
        "        direction: to indicate if the objective function is a loss to be minimized or gain to be maximized\n",
        "        n_trials: Optuna evaluation roundd\n",
        "        Returns\n",
        "        -------\n",
        "        study: Optuna study object\n",
        "        '''\n",
        "        \n",
        "        print('Running {} rounds of LGBM parameter optimisation using Optuna:'.format(MAX_EVALS))\n",
        "        fn_name = 'optuna_obj'\n",
        "        fn = getattr(self, fn_name)\n",
        "        try:\n",
        "            study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=SEED))\n",
        "            study.optimize(fn, n_trials=MAX_EVALS)\n",
        "        except Exception as e:\n",
        "            return {'exception': str(e)}\n",
        "        self.params = study.best_params\n",
        "        self.params['n_estimator'] = self.estimator\n",
        "        return study\n",
        "\n",
        "    def random_space(self):\n",
        "        '''Random search space'''\n",
        "        print('Running {} rounds of LGBM parameter optimisation using Random Search:'.format(MAX_EVALS))\n",
        "        # Dataframe to hold cv results\n",
        "        space = PARAM_GRID\n",
        "        random_results = pd.DataFrame(columns=['loss', 'params', 'iteration', 'estimators',\n",
        "                                               'time'], index=list(range(MAX_EVALS)))\n",
        "\n",
        "        # Iterate through the specified number of evaluations\n",
        "        for i in range(MAX_EVALS):\n",
        "\n",
        "            # Randomly sample parameters for gbm\n",
        "            params = {key: random.sample(value, 1)[0] for key, value in space.items()}\n",
        "            results_list = self.randomsrch_obj(params, i)\n",
        "\n",
        "            # Add results to next row in dataframe\n",
        "            random_results.loc[i, :] = results_list\n",
        "        #sort values by the loss\n",
        "        random_results.sort_values('loss', ascending = True, inplace = True)\n",
        "        self.params = random_results.loc[0, 'params']\n",
        "        self.params['n_estimator'] = self.estimator\n",
        "        return random_results\n",
        "\n",
        "    def hyperopt_obj(self, params):\n",
        "        \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization\"\"\"\n",
        "\n",
        "        optim_type = 'Hyperopt'\n",
        "        self.iteration += 1\n",
        "\n",
        "        # Retrieve the subsample if present otherwise set to 1.0\n",
        "        subsample = params['boosting_type'].get('subsample', 1.0)\n",
        "\n",
        "        # Extract the boosting type\n",
        "        params['boosting_type'] = params['boosting_type']['boosting_type']\n",
        "        params['subsample'] = subsample\n",
        "\n",
        "        # Make sure parameters that need to be integers are integers\n",
        "        for parameter_name in ['num_leaves', 'bagging_freq']:\n",
        "            params[parameter_name] = int(params[parameter_name])\n",
        "\n",
        "        # Perform n_folds cross validation\n",
        "        loss, params, n_estimators, run_time = self.lgb_crossval(params, optim_type)\n",
        "\n",
        "        # Dictionary with information for evaluation\n",
        "        return {'loss':loss, 'params':params, 'iteration':self.iteration,\n",
        "                'estimators':n_estimators, 'train_time':run_time, 'status':STATUS_OK}\n",
        "\n",
        "    def optuna_obj(self, trial):\n",
        "        '''Defining the parameters space inside the function for optuna optimization'''\n",
        "        params = {\n",
        "            'num_leaves': trial.suggest_int('num_leaves', 16, 196, 4),\n",
        "            'max_bin' : trial.suggest_int('max_bin', 254, 255, 1),\n",
        "            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
        "            'lambda_l2': trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
        "            'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'goss']),\n",
        "            # removed 'dart'\n",
        "            'learning_rate' : trial.suggest_loguniform('learning_rate', 0.05, 0.25),\n",
        "            'feature_fraction': trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n",
        "            'bagging_freq': trial.suggest_int(\"bagging_freq\", 1, 7),\n",
        "            'verbosity' : 0,\n",
        "            'objective' : OBJECTIVE_LOSS\n",
        "                }\n",
        "\n",
        "        optim_type = 'Optuna'\n",
        "        self.iteration += 1\n",
        "\n",
        "        # Make sure parameters that need to be integers are integers\n",
        "        for parameter_name in ['num_leaves', 'min_data_in_leaf',\n",
        "                               'max_bin', 'bagging_freq']:\n",
        "            params[parameter_name] = int(params[parameter_name])\n",
        "\n",
        "        # Perform n_folds cross validation\n",
        "        if params['boosting_type'] == 'goss':\n",
        "            params['subsample'] = 1\n",
        "        else:\n",
        "            params['subsample'] = trial.suggest_uniform('subsample', 0.5, 1)\n",
        "\n",
        "        loss, params, _, _ = self.lgb_crossval(params, optim_type)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def randomsrch_obj(self, params, iteration):\n",
        "        \"\"\"Random search objective function. Takes in hyperparameters and returns a list\n",
        "        of results to be saved.\"\"\"\n",
        "        optim_type = 'Random'\n",
        "        self.iteration += 1\n",
        "        random.seed(SEED)\n",
        "        # Subsampling (only applicable with 'goss')\n",
        "        subsample_dist = list(np.linspace(0.5, 1, 100))\n",
        "\n",
        "        if params['boosting_type'] == 'goss':\n",
        "            # Cannot subsample with goss\n",
        "            params['subsample'] = 1.0\n",
        "        else:\n",
        "            # Subsample supported for gdbt and dart\n",
        "            params['subsample'] = random.sample(subsample_dist, 1)[0]\n",
        "\n",
        "        # Perform n_folds cross validation\n",
        "        loss, params, n_estimators, run_time = self.lgb_crossval(params, optim_type)\n",
        "\n",
        "        # Return list of results\n",
        "        return [loss, params, iteration, n_estimators, run_time]\n",
        "\n",
        "    def train(self, x_test, y_test):\n",
        "        \"\"\"This function evaluates the model on paramters and estimators\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_test: test set; y_test: test label\"\"\"\n",
        "        self.test_x, self.test_y = x_test, y_test\n",
        "        param_df = pd.read_csv(RESULT_PATH)\n",
        "        param_df.sort_values('loss', ascending = True, inplace = True)\n",
        "\n",
        "        best = ast.literal_eval(param_df.loc[0, 'params'])\n",
        "        best['n_estimator'] = int(param_df.loc[0, 'estimators'])\n",
        "        optim_type = param_df.loc[0, 'optim_type']\n",
        "        for parameter_name in ['num_leaves', 'bagging_freq']:\n",
        "            best[parameter_name] = int(best[parameter_name])\n",
        "        self.gbm = lgb.train(best, self.train_set,\n",
        "                             feature_name=['f' + str(i + 1) for i in range(self.x_train.shape[-1])])\n",
        "        self.pred = self.gbm.predict(x_test)\n",
        "        print(\"Model will be trained with best parameters obtained from {} ... \\n\\n\\n\".format(optim_type))\n",
        "        print(\"Model trained on the following parameters: \\n{}\".format(best))\n",
        "        print('Plotting feature importances...')\n",
        "        ax = lgb.plot_importance(self.gbm, max_num_features=10)\n",
        "        plt.savefig('feature_importance.png')\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"This function generates the evaluation report for the model\"\"\"\n",
        "        pred = self.pred\n",
        "        print('check pred')\n",
        "        (self.fpr, self.tpr, self.thresholds) = roc_curve(y_true=self.test_y, y_score=pred)\n",
        "        print('fpr, tpr, thresh check')\n",
        "        self.fnr = 1- self.tpr\n",
        "        print('fnr check')\n",
        "        self.roc_auc = auc(self.fpr, self.tpr)\n",
        "        print('roc_auc check')\n",
        "        self.precision, self.recall, _ = precision_recall_curve(self.test_y, pred)\n",
        "        print('precision recall check')\n",
        "        self.pr_auc = auc(self.recall, self.precision)\n",
        "        print('pr_auc check')\n",
        "        eval_list = ['feature_importance','roc', 'prcurve', 'fpr_fnr']\n",
        "        for eval_name in eval_list:\n",
        "            try:\n",
        "                func = getattr(self,eval_name)\n",
        "                func()\n",
        "            except:\n",
        "                print('Not valid evaluation type')\n",
        "\n",
        "    def feature_importance(self):\n",
        "        print('Plotting feature importances...')\n",
        "        ax = lgb.plot_importance(self.gbm, max_num_features=10)\n",
        "        plt.savefig('feature_importance.png')\n",
        "              \n",
        "\n",
        "    def roc(self):\n",
        "        fpr, tpr, roc = self.fpr, self.tpr, self.roc_auc\n",
        "\n",
        "        plt.figure(figsize=(16, 8))\n",
        "        lw = 2\n",
        "\n",
        "        plt.plot(fpr, tpr, color='darkorange',\n",
        "                 lw=lw, label='ROC curve (area = %0.2f)' % self.roc_auc, alpha=0.5)\n",
        "\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--', alpha=0.5)\n",
        "\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xticks(fontsize=16)\n",
        "        plt.yticks(fontsize=16)\n",
        "        plt.grid(True)\n",
        "        plt.xlabel('False Positive Rate', fontsize=16)\n",
        "        plt.ylabel('True Positive Rate', fontsize=16)\n",
        "        plt.title('Receiver operating characteristic', fontsize=20)\n",
        "        plt.legend(loc=\"lower right\", fontsize=16)\n",
        "        plt.savefig('roc.png')\n",
        "        \n",
        "    def prcurve(self):\n",
        "        '''\n",
        "        A class method to output the precision recall curve for an instance\n",
        "        '''\n",
        "        recall, precision, pr_auc = self.recall, self.precision, self.pr_auc\n",
        "        test_y =  self.test_y\n",
        "        # plot the precision-recall curves\n",
        "        no_skill = len(test_y[test_y==1]) / len(test_y)\n",
        "        plt.figure(figsize = (16,8))\n",
        "        plt.plot([0, 1], [no_skill, no_skill], color='navy', linestyle='--',\n",
        "                 alpha=0.5)\n",
        "        plt.plot(recall, precision, color='darkorange',\n",
        "                 label='ROC curve (area = %0.2f)'% pr_auc, alpha=0.5)\n",
        "        # axis labels\n",
        "        plt.title('Precision Recall Curve', size = 20)\n",
        "        plt.xlabel('Recall', fontsize=16)\n",
        "        plt.ylabel('Precision', fontsize=16)\n",
        "        plt.grid(True)\n",
        "        plt.xticks(fontsize=16)\n",
        "        plt.yticks(fontsize=16)\n",
        "        # show the legend\n",
        "        plt.legend(fontsize=16)\n",
        "        plt.savefig('prcurve.png')\n",
        "    def fpr_fnr(self):\n",
        "        '''\n",
        "        A class method to output the fpr_fnr curve for an instance\n",
        "        '''\n",
        "        lw = 2\n",
        "        fpr, fnr, thresholds = self.fpr, self.fnr, self.thresholds\n",
        "        plt.figure(figsize = (16,8))\n",
        "        plt.plot(thresholds, fpr, color='blue', lw=lw, label='FPR', alpha=0.5)\n",
        "        plt.plot(thresholds, fnr, color='green', lw=lw, label='FNR', alpha=0.5)\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xticks(fontsize=16)\n",
        "        plt.yticks(fontsize=16)\n",
        "        plt.grid(True)\n",
        "        plt.xlabel('Threshold', fontsize=16)\n",
        "        plt.ylabel('Error Rate', fontsize=16)\n",
        "        plt.title('FPR-FNR curves', fontsize=20)\n",
        "        plt.legend(loc=\"lower left\", fontsize=16)\n",
        "        plt.savefig('fpr-fnr.png')\n",
        "\n",
        "\n",
        "\n",
        "obj = Lgbmclass(train_X, train_y)\n",
        "obj.parameter_tuning('hyperopt_space')\n",
        "obj.train(train_X, train_y)\n",
        "obj.evaluate()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-73508942ec63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhyperopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTATUS_OK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTATUS_FAIL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightgbm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlgbo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oStenFAvQe8L",
        "colab_type": "code",
        "outputId": "fdbbbb7b-2e99-42df-e774-b10f1a2d56c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "obj.hyperopt_space()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running 100 rounds of LGBM parameter optimisation using Hyperopt:\n",
            "  0%|          | 0/100 [00:00<?, ?it/s, best loss: ?]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'exception': 'Found input variables with inconsistent numbers of samples: [105000, 1050000]',\n",
              " 'status': 'fail'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6S7RbLFuBZn",
        "colab_type": "code",
        "outputId": "bcc964e9-6200-4f96-fe07-7cd5ddce48fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "import ast \n",
        "import csv\n",
        "from timeit import default_timer as timer\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import auc, accuracy_score, roc_auc_score, roc_curve, confusion_matrix, precision_recall_curve\n",
        "from hyperopt import STATUS_OK, STATUS_FAIL, hp, tpe, Trials, fmin\n",
        "import optuna.integration.lightgbm as lgbo\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def objective(params, n_folds = N_FOLDS):\n",
        "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization\"\"\"\n",
        "    \n",
        "    # Keep track of evals\n",
        "    global ITERATION\n",
        "    \n",
        "    ITERATION += 1\n",
        "    \n",
        "    # Retrieve the subsample if present otherwise set to 1.0\n",
        "    subsample = params['boosting_type'].get('subsample', 1.0)\n",
        "    \n",
        "    # Extract the boosting type\n",
        "    params['boosting_type'] = params['boosting_type']['boosting_type']\n",
        "    params['subsample'] = subsample\n",
        "    \n",
        "    # Make sure parameters that need to be integers are integers\n",
        "    for parameter_name in ['num_leaves', 'bagging_freq']:\n",
        "        params[parameter_name] = int(params[parameter_name])\n",
        "    \n",
        "\n",
        "    loss, params, ITERATION, n_estimators, run_time = lgbcrossval(params, n_folds)\n",
        "    # Dictionary with information for evaluation\n",
        "    return {'loss': loss, 'params': params, 'iteration': ITERATION,\n",
        "            'estimators': n_estimators, \n",
        "            'train_time': run_time, 'status': STATUS_OK}\n",
        "\n",
        "def lgbcrossval(params, n_folds):\n",
        "    start = timer()\n",
        "    \n",
        "    # Perform n_folds cross validation\n",
        "    cv_results = lgb.cv(params, train_set, num_boost_round = 10000, nfold = n_folds, \n",
        "                    early_stopping_rounds = 100, metrics = ['auc','binary','xentropy'], seed = 50,\n",
        "                    )\n",
        "    \n",
        "    run_time = timer() - start\n",
        "    \n",
        "    # Extract the best score\n",
        "    best_score = np.max(cv_results['auc-mean'])\n",
        "    \n",
        "    # Loss must be minimized\n",
        "    loss = 1 - best_score\n",
        "    \n",
        "    # Boosting rounds that returned the highest cv score\n",
        "    n_estimators = int(np.argmax(cv_results['auc-mean']) + 1)\n",
        "\n",
        "    # Write to the csv file ('a' means append)\n",
        "    of_connection = open(out_file, 'a')\n",
        "    writer = csv.writer(of_connection)\n",
        "    writer.writerow([loss, params, ITERATION, n_estimators, run_time])\n",
        "    return loss, params, ITERATION, n_estimators, run_time\n",
        "\n",
        "# Hyperopt Space\n",
        "space = {    \n",
        "            'boosting_type': hp.choice('boosting_type', [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n",
        "                                                         #{'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n",
        "                                                         {'boosting_type': 'goss', 'subsample': 1.0}]),\n",
        "            'objective' : 'binary',\n",
        "\n",
        "            'num_leaves': hp.quniform('num_leaves', 16, 196, 4),\n",
        "\n",
        "            'lambda_l1': hp.uniform('lambda_l1', 0.0, 1.0),\n",
        "            \n",
        "            'lambda_l2': hp.uniform(\"lambda_l2\", 0.0, 1.0),\n",
        "            \n",
        "            'learning_rate' : hp.loguniform('learning_rate', np.log(0.05), np.log(0.25)),\n",
        "\n",
        "            'feature_fraction': hp.uniform('feature_fraction', 0.4, 1.0),\n",
        "                     \n",
        "            'bagging_freq': hp.uniform('bagging_freq', 1, 7),\n",
        "                     \n",
        "            'verbosity' : 0,\n",
        "         \n",
        "            'device' : 'gpu'\n",
        "            \n",
        "            }\n",
        "\n",
        "# optimization algorithm\n",
        "tpe_algorithm = tpe.suggest\n",
        "\n",
        "# Keep track of results\n",
        "bayes_trials = Trials()\n",
        "\n",
        "# File to save first results\n",
        "out_file = 'gbm_trials.csv'\n",
        "of_connection = open(out_file, 'w')\n",
        "writer = csv.writer(of_connection)\n",
        "\n",
        "# Write the headers to the file\n",
        "writer.writerow(['loss', 'params', 'iteration', 'estimators', 'train_time'])\n",
        "of_connection.close()\n",
        "\n",
        "# Global variable\n",
        "global  ITERATION\n",
        "\n",
        "ITERATION = 0\n",
        "\n",
        "train_set = lgb.Dataset(data=train_X, label = train_y)\n",
        "\n",
        "# Run optimization\n",
        "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
        "            max_evals = 10, trials = bayes_trials, rstate = np.random.RandomState(50))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a638f990abb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhyperopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTATUS_OK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTATUS_FAIL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightgbm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlgbo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kobZaulUyZiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform n_folds cross validation\n",
        "'''params = {max_bin : 255,\n",
        "          num_leaves : 255,\n",
        "          num_iterations : 500,\n",
        "          learning_rate : 0.1\n",
        "          num_thread=16,\n",
        "          min_data_in_leaf=1,\n",
        "          min_sum_hessian_in_leaf100\n",
        "          device: 'gpu'}'''\n",
        "params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'metric': {'l2', 'l1'},\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': 0,\n",
        "    'device': 'gpu'\n",
        "}\n",
        "cv_results = lgb.cv(params, train_set, num_boost_round = 10000, nfold = 3, \n",
        "                early_stopping_rounds = 100, metrics = ['auc','binary','xentropy'], seed = 50,\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xag7G_nm-Pee",
        "colab_type": "code",
        "outputId": "ea628797-47e3-4b09-cb0c-892695ac2037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cv_results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'auc-mean': [0.6235088438100641,\n",
              "  0.6399884054707041,\n",
              "  0.644268711959402,\n",
              "  0.6482776184192246,\n",
              "  0.6495378868386729,\n",
              "  0.6523837987401494,\n",
              "  0.6535627543770622,\n",
              "  0.6555396896920428,\n",
              "  0.6564199075512491,\n",
              "  0.6567035921783401,\n",
              "  0.6571618064360764,\n",
              "  0.6580433842763572,\n",
              "  0.6594770849511158,\n",
              "  0.6599815123594032,\n",
              "  0.6605888741200733,\n",
              "  0.6614762249189572,\n",
              "  0.6616732813176114,\n",
              "  0.6627953033750784,\n",
              "  0.6631025013160642,\n",
              "  0.6637209820548798,\n",
              "  0.664826318001718,\n",
              "  0.6652002178005051,\n",
              "  0.6655041634704109,\n",
              "  0.6665019129577869,\n",
              "  0.6670321385136893,\n",
              "  0.6674029700292011,\n",
              "  0.6678266912141559,\n",
              "  0.6682273697009066,\n",
              "  0.6684398199278924,\n",
              "  0.6685974150370741,\n",
              "  0.668842846579477,\n",
              "  0.6691065065635143,\n",
              "  0.6697122113420241,\n",
              "  0.6703015422978115,\n",
              "  0.6707208101043544,\n",
              "  0.6712566961515368,\n",
              "  0.6719051592591452,\n",
              "  0.6725352661709754,\n",
              "  0.6732005993770175,\n",
              "  0.6736385848034537,\n",
              "  0.6741409965352553,\n",
              "  0.6750839744363827,\n",
              "  0.6754699557370546,\n",
              "  0.6762253359597663,\n",
              "  0.6769192134815502,\n",
              "  0.677324496577045,\n",
              "  0.67755156155047,\n",
              "  0.6777913189526457,\n",
              "  0.6782365022877469,\n",
              "  0.6786098796048446,\n",
              "  0.6789191985921869,\n",
              "  0.6795122830083861,\n",
              "  0.6801187908909347,\n",
              "  0.6805720658677478,\n",
              "  0.680768043453635,\n",
              "  0.6810766516038198,\n",
              "  0.6813745688068354,\n",
              "  0.6816480844224887,\n",
              "  0.6818651365284291,\n",
              "  0.6820943918848611,\n",
              "  0.6824723503345141,\n",
              "  0.6828791975993297,\n",
              "  0.6831059420954803,\n",
              "  0.6833668608250117,\n",
              "  0.6835520835866578,\n",
              "  0.68376238110173,\n",
              "  0.6842980618687503,\n",
              "  0.6844550400455299,\n",
              "  0.6848567263705237,\n",
              "  0.6852095167079533,\n",
              "  0.685565254126009,\n",
              "  0.6858840847915039,\n",
              "  0.6860950440075243,\n",
              "  0.6864999056235374,\n",
              "  0.6867309069532636,\n",
              "  0.6870476760818107,\n",
              "  0.6874324033172367,\n",
              "  0.687515919585768,\n",
              "  0.6875854931820081,\n",
              "  0.6879329407756424,\n",
              "  0.6881304656061561,\n",
              "  0.6884218662559386,\n",
              "  0.6886192322127099,\n",
              "  0.6888384452199318,\n",
              "  0.6888641259866027,\n",
              "  0.6890574551308232,\n",
              "  0.689292471434785,\n",
              "  0.68953868205908,\n",
              "  0.6896346079501811,\n",
              "  0.6896948183688113,\n",
              "  0.6899015431355293,\n",
              "  0.690091381425077,\n",
              "  0.6903114777921457,\n",
              "  0.6904778660989125,\n",
              "  0.6904703973951397,\n",
              "  0.6906460833645709,\n",
              "  0.6908401146106224,\n",
              "  0.6909080088404295,\n",
              "  0.6909286613350428,\n",
              "  0.6910652250548438,\n",
              "  0.6911846795466604,\n",
              "  0.6913696599030086,\n",
              "  0.6913902938350541,\n",
              "  0.69154312273196,\n",
              "  0.6916406548311386,\n",
              "  0.6918781159344786,\n",
              "  0.6920512631997765,\n",
              "  0.6921883881642761,\n",
              "  0.6923135348129299,\n",
              "  0.69245937435708,\n",
              "  0.6926297230422946,\n",
              "  0.6928060368632857,\n",
              "  0.6929056861871153,\n",
              "  0.6929185467708924,\n",
              "  0.6929966569645147,\n",
              "  0.6931585149128088,\n",
              "  0.6932649188275067,\n",
              "  0.6933540398996151,\n",
              "  0.6935364040258113,\n",
              "  0.6935628764314054,\n",
              "  0.6937086864938301,\n",
              "  0.6937946344587509,\n",
              "  0.6938947598578503,\n",
              "  0.6939942246479175,\n",
              "  0.6940923081645529,\n",
              "  0.6942391468116199,\n",
              "  0.6943686392849835,\n",
              "  0.694488523991608,\n",
              "  0.6945335797114575,\n",
              "  0.6946185056432302,\n",
              "  0.6946713532739164,\n",
              "  0.694723051117311,\n",
              "  0.6947952267487983,\n",
              "  0.6948935395677425,\n",
              "  0.6950608538190707,\n",
              "  0.6951534570105254,\n",
              "  0.6951807199631271,\n",
              "  0.6952352174785208,\n",
              "  0.6952231081327843,\n",
              "  0.6952245691160662,\n",
              "  0.6953374961354917,\n",
              "  0.6954188111018587,\n",
              "  0.6954758167477445,\n",
              "  0.6955134463485358,\n",
              "  0.6955540743499938,\n",
              "  0.6956971972077426,\n",
              "  0.6957255771901628,\n",
              "  0.695840451095382,\n",
              "  0.695921886172482,\n",
              "  0.6960260418326379,\n",
              "  0.6961875001397652,\n",
              "  0.696311952329998,\n",
              "  0.6964103634213603,\n",
              "  0.6965052454409983,\n",
              "  0.6966242533392705,\n",
              "  0.6966619004107141,\n",
              "  0.6966711849703894,\n",
              "  0.6966319262312896,\n",
              "  0.6966700482860871,\n",
              "  0.6966820364291748,\n",
              "  0.6966903972281203,\n",
              "  0.6967350969834333,\n",
              "  0.6967000661421417,\n",
              "  0.6966944755334699,\n",
              "  0.6968095306967045,\n",
              "  0.696813515097299,\n",
              "  0.6968388366236861,\n",
              "  0.6968640260282667,\n",
              "  0.6968807978542825,\n",
              "  0.6968974670402172,\n",
              "  0.696958468005855,\n",
              "  0.6969942337064506,\n",
              "  0.6970784892019489,\n",
              "  0.6971412088429872,\n",
              "  0.6971615173841373,\n",
              "  0.6972349978549534,\n",
              "  0.6972784648373777,\n",
              "  0.6973444165490542,\n",
              "  0.697379625372614,\n",
              "  0.6973877219279466,\n",
              "  0.6974478722912103,\n",
              "  0.6975225647885157,\n",
              "  0.6975811657233213,\n",
              "  0.6976840383824636,\n",
              "  0.6977023170522231,\n",
              "  0.6977781156601894,\n",
              "  0.6977964390984948,\n",
              "  0.697799250781567,\n",
              "  0.6979257077291283,\n",
              "  0.69790991425963,\n",
              "  0.6979278446082633,\n",
              "  0.6979800250784223,\n",
              "  0.6980357728373568,\n",
              "  0.6980789591974318,\n",
              "  0.6980698711825926,\n",
              "  0.6981617438824066,\n",
              "  0.6982214006997486,\n",
              "  0.6982551452642793,\n",
              "  0.6982749711786652,\n",
              "  0.6982918663911616,\n",
              "  0.6982771473667674,\n",
              "  0.6983771461036392,\n",
              "  0.698367919415499,\n",
              "  0.6984210422089556,\n",
              "  0.6984678176961213,\n",
              "  0.6984664790074042,\n",
              "  0.6985430048312642,\n",
              "  0.698646457297673,\n",
              "  0.6986378944943127,\n",
              "  0.6986289320497855,\n",
              "  0.698696349112407,\n",
              "  0.6988015824934143,\n",
              "  0.6987796382624613,\n",
              "  0.6988095196290462,\n",
              "  0.6988565986687924,\n",
              "  0.6989436527443739,\n",
              "  0.6989744775261725,\n",
              "  0.6989725197212214,\n",
              "  0.6989997357214457,\n",
              "  0.6989715795817554,\n",
              "  0.6990283024214604,\n",
              "  0.6990171091930432,\n",
              "  0.699064036456499,\n",
              "  0.699088305376097,\n",
              "  0.6991060763050297,\n",
              "  0.6992721250259963,\n",
              "  0.6994672765776139,\n",
              "  0.6994830132674927,\n",
              "  0.6995308042363116,\n",
              "  0.6995726442642591,\n",
              "  0.699631235371823,\n",
              "  0.6996571880255171,\n",
              "  0.6996405079204249,\n",
              "  0.6997191804506618,\n",
              "  0.6997310867000678,\n",
              "  0.6997439483757605,\n",
              "  0.6998574672135259,\n",
              "  0.6999102613403401,\n",
              "  0.6999147851473181,\n",
              "  0.6999491193463573,\n",
              "  0.699923507370379,\n",
              "  0.6999448816213073,\n",
              "  0.6999232005420515,\n",
              "  0.6999851241765033,\n",
              "  0.6999863285595825,\n",
              "  0.7000337591962019,\n",
              "  0.7000112799265282,\n",
              "  0.7000372009146655,\n",
              "  0.7000911917811483,\n",
              "  0.700044660883112,\n",
              "  0.7000324150479059,\n",
              "  0.7000522824550907,\n",
              "  0.7000439009097458,\n",
              "  0.7000547283463847,\n",
              "  0.7000414080660743,\n",
              "  0.700034914443072,\n",
              "  0.7000586243018029,\n",
              "  0.700056106344069,\n",
              "  0.7000997720551609,\n",
              "  0.7001625004315252,\n",
              "  0.7001839642195454,\n",
              "  0.7002271986239137,\n",
              "  0.7002551451157787,\n",
              "  0.7002930225814468,\n",
              "  0.7002929614341644,\n",
              "  0.7002836845178995,\n",
              "  0.700322603671199,\n",
              "  0.7003001571589981,\n",
              "  0.7003109354594278,\n",
              "  0.7003485366704095,\n",
              "  0.7003924153050739,\n",
              "  0.7004105290955541,\n",
              "  0.7003970908883422,\n",
              "  0.7004204731723461,\n",
              "  0.7004491053873062,\n",
              "  0.7005101139963544,\n",
              "  0.7005269142121798,\n",
              "  0.7005350042160178,\n",
              "  0.700526478537793,\n",
              "  0.7005091334560053,\n",
              "  0.7005638602737078,\n",
              "  0.7005776074930788,\n",
              "  0.7005752096460781,\n",
              "  0.7006119558870227,\n",
              "  0.7006372937921462,\n",
              "  0.700676717410525,\n",
              "  0.7006854243467654,\n",
              "  0.7007011271872559,\n",
              "  0.7006751668901513,\n",
              "  0.7007018893444537,\n",
              "  0.7007375775190304,\n",
              "  0.7007416492728854,\n",
              "  0.7007650828769302,\n",
              "  0.7007640029722472,\n",
              "  0.7007993886861593,\n",
              "  0.7008378514186728,\n",
              "  0.7008823742836333,\n",
              "  0.7009213054480062,\n",
              "  0.700949706176826,\n",
              "  0.7009362133738261,\n",
              "  0.7009171845579422,\n",
              "  0.7008984188937554,\n",
              "  0.7008884693573846,\n",
              "  0.7009457632690301,\n",
              "  0.7009182841171088,\n",
              "  0.7009064411988165,\n",
              "  0.7008953877356161,\n",
              "  0.7009445971030025,\n",
              "  0.7009867483269404,\n",
              "  0.7009659506075314,\n",
              "  0.7010214996377985,\n",
              "  0.7010519444329137,\n",
              "  0.7010577479651583,\n",
              "  0.701144966920019,\n",
              "  0.7011103957752609,\n",
              "  0.7011270736965217,\n",
              "  0.7011284713486896,\n",
              "  0.7011187620337852,\n",
              "  0.7010981117230032,\n",
              "  0.7011245732094399,\n",
              "  0.7011805098698002,\n",
              "  0.7012632072015146,\n",
              "  0.7012462977941135,\n",
              "  0.7013118662433597,\n",
              "  0.7013776902008929,\n",
              "  0.7013921810148939,\n",
              "  0.7013649988640581,\n",
              "  0.7013687878117324,\n",
              "  0.701361537491111,\n",
              "  0.7013363994065709,\n",
              "  0.701363736609444,\n",
              "  0.7014140062270297,\n",
              "  0.7013769957424719,\n",
              "  0.7013432599132673,\n",
              "  0.7013733858689819,\n",
              "  0.701394049282753,\n",
              "  0.7014409416049044,\n",
              "  0.701428123605842,\n",
              "  0.7014462286609962,\n",
              "  0.701479413072744,\n",
              "  0.7014949597692812,\n",
              "  0.7014760226743207,\n",
              "  0.7014696699084512,\n",
              "  0.701447380632119,\n",
              "  0.7014326212068419,\n",
              "  0.7013933111477016,\n",
              "  0.7013703623542528,\n",
              "  0.7014198239541788,\n",
              "  0.701395997260462,\n",
              "  0.7014085673946479,\n",
              "  0.7014060243228514,\n",
              "  0.7013977224873568,\n",
              "  0.7013862246144437,\n",
              "  0.7013943866847215,\n",
              "  0.7014120320433422,\n",
              "  0.7014632996719291,\n",
              "  0.7014699035784228],\n",
              " 'auc-stdv': [0.0006684642021263255,\n",
              "  0.0006409399558060676,\n",
              "  0.001646341738418727,\n",
              "  0.0013075102996119392,\n",
              "  0.0011628171216340227,\n",
              "  0.0013830014567212195,\n",
              "  0.0016107832414954384,\n",
              "  0.0015721949671389885,\n",
              "  0.0014889216144873655,\n",
              "  0.0014010730323200517,\n",
              "  0.0011439373448669586,\n",
              "  0.0011759083568121421,\n",
              "  0.0011828648315222617,\n",
              "  0.0013748807826487026,\n",
              "  0.0012684990018943245,\n",
              "  0.001406833908101553,\n",
              "  0.0014847078075439842,\n",
              "  0.0010387256322133874,\n",
              "  0.0010883563855287478,\n",
              "  0.0008674848006094051,\n",
              "  0.0011064356350694048,\n",
              "  0.0007068095857539123,\n",
              "  0.0009035004716289717,\n",
              "  0.0007888299960651087,\n",
              "  0.0005875680662130124,\n",
              "  0.00048389858830545884,\n",
              "  0.00045149826782919415,\n",
              "  0.0004634495805774527,\n",
              "  0.0004593551371827031,\n",
              "  0.0005928995125009638,\n",
              "  0.0005590289535676673,\n",
              "  0.00048565021957821294,\n",
              "  0.0009026484070013251,\n",
              "  0.001067422877879544,\n",
              "  0.0008810067391435008,\n",
              "  0.0009268626903578317,\n",
              "  0.0010190262561901758,\n",
              "  0.000991054526016723,\n",
              "  0.0007568813897044173,\n",
              "  0.0007986083711285943,\n",
              "  0.0010979425313958659,\n",
              "  0.0009436259284225599,\n",
              "  0.0011375892400484965,\n",
              "  0.0010453710140513927,\n",
              "  0.0010961773028995952,\n",
              "  0.0010632102075919918,\n",
              "  0.0009775703134014403,\n",
              "  0.001132998335454009,\n",
              "  0.0012583962170963298,\n",
              "  0.0012251710145850928,\n",
              "  0.001213010928948797,\n",
              "  0.0012198210514483866,\n",
              "  0.0013168833122533692,\n",
              "  0.0013105154795653691,\n",
              "  0.001345506124483691,\n",
              "  0.0013718065966554051,\n",
              "  0.0013341221752247979,\n",
              "  0.0013459188507314024,\n",
              "  0.001250919871868475,\n",
              "  0.0011546576364797458,\n",
              "  0.001168761541064064,\n",
              "  0.0010653762166515626,\n",
              "  0.0010965826817251835,\n",
              "  0.001012832787944697,\n",
              "  0.0009896410109576127,\n",
              "  0.0009355296408290646,\n",
              "  0.0009945116196581718,\n",
              "  0.0009950704312082371,\n",
              "  0.0010943802631897999,\n",
              "  0.000980504197095743,\n",
              "  0.0008713648513948326,\n",
              "  0.0009118667103871968,\n",
              "  0.0009716841565021387,\n",
              "  0.0009911060378333452,\n",
              "  0.000940767665075049,\n",
              "  0.0008883627347375331,\n",
              "  0.0009691590066994387,\n",
              "  0.0009258437382254668,\n",
              "  0.0009108180998943017,\n",
              "  0.001020496039432397,\n",
              "  0.0009977268509254319,\n",
              "  0.0010118803488269107,\n",
              "  0.0009848138777200476,\n",
              "  0.0009992775245046368,\n",
              "  0.0010580309572545397,\n",
              "  0.0011410092166087196,\n",
              "  0.0010579473907032335,\n",
              "  0.0010884716857916122,\n",
              "  0.0010706951499175215,\n",
              "  0.0010460268083202259,\n",
              "  0.0011007408188042158,\n",
              "  0.0011187946585510902,\n",
              "  0.0012254447127282198,\n",
              "  0.001227256702000654,\n",
              "  0.0012782657726108063,\n",
              "  0.0014145237285000717,\n",
              "  0.0015156184609832367,\n",
              "  0.0015909716459708541,\n",
              "  0.0015681044946646526,\n",
              "  0.0014768063386885121,\n",
              "  0.0014313382090652364,\n",
              "  0.0014791272721474606,\n",
              "  0.0014377997000016532,\n",
              "  0.0014644690624150715,\n",
              "  0.0013934202700922724,\n",
              "  0.0014034636825870459,\n",
              "  0.0014041192106796245,\n",
              "  0.0014173850225281538,\n",
              "  0.0014975409102185367,\n",
              "  0.00146707237031478,\n",
              "  0.0014103189193951037,\n",
              "  0.0014213168542399565,\n",
              "  0.0014747473671182516,\n",
              "  0.0014123524603412534,\n",
              "  0.0014093574794141882,\n",
              "  0.001400819983287834,\n",
              "  0.0013716363669538578,\n",
              "  0.0013484502925973562,\n",
              "  0.0012389361420393325,\n",
              "  0.001200853909157723,\n",
              "  0.0011695260731526618,\n",
              "  0.0011550782586592905,\n",
              "  0.0011620555117590231,\n",
              "  0.0012694119634067507,\n",
              "  0.0012818467392286981,\n",
              "  0.0013178598221021893,\n",
              "  0.0013065820241201212,\n",
              "  0.0012950907289542063,\n",
              "  0.0012905460833637465,\n",
              "  0.0013360163670433417,\n",
              "  0.0013689603455134336,\n",
              "  0.0013799906258513764,\n",
              "  0.001426827532045045,\n",
              "  0.0014770471607161437,\n",
              "  0.0015680195841553525,\n",
              "  0.001542750007087395,\n",
              "  0.001512604213005619,\n",
              "  0.0015368944484597327,\n",
              "  0.0015047398201010016,\n",
              "  0.001525521054932386,\n",
              "  0.0014596897437281794,\n",
              "  0.0014444487220623566,\n",
              "  0.0014922793276993245,\n",
              "  0.001434844799931909,\n",
              "  0.0014226845597796692,\n",
              "  0.0014326879858885665,\n",
              "  0.0014545825728700674,\n",
              "  0.0015082265010998696,\n",
              "  0.001546888744269238,\n",
              "  0.0015275355762024347,\n",
              "  0.001577817230266219,\n",
              "  0.0017252927328403461,\n",
              "  0.0017900565644395916,\n",
              "  0.0018741255187664264,\n",
              "  0.001958514454756305,\n",
              "  0.0019319779235278337,\n",
              "  0.001970586729763118,\n",
              "  0.0019649303609539284,\n",
              "  0.001974565225881502,\n",
              "  0.0019726083313057136,\n",
              "  0.0019638132233922275,\n",
              "  0.0019278255843299017,\n",
              "  0.0019065203788915846,\n",
              "  0.001863805234089495,\n",
              "  0.001893294916158818,\n",
              "  0.0019336933531991652,\n",
              "  0.0019479734344532468,\n",
              "  0.0019531676728748964,\n",
              "  0.001861044817914945,\n",
              "  0.0018520920642652826,\n",
              "  0.0018755676561468572,\n",
              "  0.001890218436828402,\n",
              "  0.0018584656642279243,\n",
              "  0.001824950294934445,\n",
              "  0.0018688576401983258,\n",
              "  0.0018144159151451233,\n",
              "  0.0018124354226835453,\n",
              "  0.001770435826036575,\n",
              "  0.0017319204525792463,\n",
              "  0.0016954134302489623,\n",
              "  0.0016988306404950944,\n",
              "  0.0016999147249862805,\n",
              "  0.0016997918315816824,\n",
              "  0.0016389143684240489,\n",
              "  0.0016691798553514709,\n",
              "  0.0017143289046826795,\n",
              "  0.0016863158146775835,\n",
              "  0.0016453987604212138,\n",
              "  0.001636935868543753,\n",
              "  0.001596323620563998,\n",
              "  0.0015377256629707388,\n",
              "  0.0015822165638521915,\n",
              "  0.0015346376713945965,\n",
              "  0.0014768659655753818,\n",
              "  0.0014657131847639186,\n",
              "  0.001431477563063032,\n",
              "  0.0014379224994302707,\n",
              "  0.0014365225081277892,\n",
              "  0.0014211171028306272,\n",
              "  0.0014422136229580327,\n",
              "  0.0014210753364943855,\n",
              "  0.0014127551844487036,\n",
              "  0.0014131167468902398,\n",
              "  0.0013933500805104898,\n",
              "  0.0013691631516692707,\n",
              "  0.001342122432668138,\n",
              "  0.001309275220630195,\n",
              "  0.0012955575690327605,\n",
              "  0.0013032876834974982,\n",
              "  0.0013334378430509383,\n",
              "  0.0012810983338414287,\n",
              "  0.00129909408550751,\n",
              "  0.0012983147609733725,\n",
              "  0.0013331157566230216,\n",
              "  0.0012949083369367454,\n",
              "  0.0013723136136695531,\n",
              "  0.0014249369771931252,\n",
              "  0.0014194516516349254,\n",
              "  0.0014592545629550401,\n",
              "  0.0014444595421114284,\n",
              "  0.0014624532220728704,\n",
              "  0.0014806324889868542,\n",
              "  0.00147667330255902,\n",
              "  0.0014728612503299551,\n",
              "  0.0014803100670746111,\n",
              "  0.001589165927948675,\n",
              "  0.0016394147597623265,\n",
              "  0.0016409608341756137,\n",
              "  0.0016263125381534077,\n",
              "  0.001654045615542594,\n",
              "  0.0016282194850390677,\n",
              "  0.0016582840666934235,\n",
              "  0.0016641376645901735,\n",
              "  0.0016559638554036378,\n",
              "  0.0016508916828590467,\n",
              "  0.00164461402871045,\n",
              "  0.0016036131085116214,\n",
              "  0.0015614575119907773,\n",
              "  0.0015087231542576418,\n",
              "  0.0014953839794215255,\n",
              "  0.0015183146316722785,\n",
              "  0.0015401203620572601,\n",
              "  0.001521877165171771,\n",
              "  0.0014904907751977807,\n",
              "  0.0015134714788015956,\n",
              "  0.0015434005799917377,\n",
              "  0.0015569444737658284,\n",
              "  0.001553862691085914,\n",
              "  0.001601240651481082,\n",
              "  0.0016320315805074472,\n",
              "  0.001645890947680896,\n",
              "  0.0016511676535685671,\n",
              "  0.0016806042682046408,\n",
              "  0.0016901303381457345,\n",
              "  0.001670530354942372,\n",
              "  0.0016595219353891784,\n",
              "  0.0016225629272511232,\n",
              "  0.0016433102439884215,\n",
              "  0.0015961441074795449,\n",
              "  0.0016209820607073235,\n",
              "  0.001658909849336373,\n",
              "  0.0016741626140152606,\n",
              "  0.0017051559557669479,\n",
              "  0.0017141526793173678,\n",
              "  0.0016990139820409473,\n",
              "  0.0017095911919228915,\n",
              "  0.00164480003619212,\n",
              "  0.001614118971958814,\n",
              "  0.0016063858735404501,\n",
              "  0.001577168252821169,\n",
              "  0.0015245450827534688,\n",
              "  0.0014841407181692085,\n",
              "  0.0014914256095411946,\n",
              "  0.0014811584309290728,\n",
              "  0.0014548055087477627,\n",
              "  0.0014584681137205435,\n",
              "  0.0014275101057677421,\n",
              "  0.001443576200108259,\n",
              "  0.0014464101032023173,\n",
              "  0.0014711204915282224,\n",
              "  0.0015202422079074543,\n",
              "  0.001514265379062476,\n",
              "  0.0015037367871004763,\n",
              "  0.0015222251946378608,\n",
              "  0.001523969996138183,\n",
              "  0.0014998642143708159,\n",
              "  0.0014816504065380159,\n",
              "  0.0014890605484709789,\n",
              "  0.001489930666303774,\n",
              "  0.0014661974945266941,\n",
              "  0.0014254240226452657,\n",
              "  0.0014453519916986027,\n",
              "  0.0014201972089254646,\n",
              "  0.0013939472880529354,\n",
              "  0.0013915480561129776,\n",
              "  0.0013904727770193788,\n",
              "  0.0013611444605386698,\n",
              "  0.001376383223656545,\n",
              "  0.001376703348001263,\n",
              "  0.0013468584460896394,\n",
              "  0.0013556196346120664,\n",
              "  0.0013760386874748969,\n",
              "  0.0013813844328992085,\n",
              "  0.0013974282507369922,\n",
              "  0.0013838230018071828,\n",
              "  0.001378079030013753,\n",
              "  0.0013744618266259365,\n",
              "  0.0014004037957039983,\n",
              "  0.0014498318108292219,\n",
              "  0.0013970918302402846,\n",
              "  0.0014427685006840841,\n",
              "  0.0014220841669164669,\n",
              "  0.001385633569193294,\n",
              "  0.0014091311898813872,\n",
              "  0.0013990656334881672,\n",
              "  0.0014107956020365923,\n",
              "  0.001377607901344397,\n",
              "  0.0013690004267795342,\n",
              "  0.001359001820194349,\n",
              "  0.0013144267231438623,\n",
              "  0.0013289145527797214,\n",
              "  0.0013317908015458822,\n",
              "  0.0013377021023298727,\n",
              "  0.0013509660117589336,\n",
              "  0.0014175354011794455,\n",
              "  0.0014385582234556456,\n",
              "  0.001425196133628086,\n",
              "  0.0014554478796527045,\n",
              "  0.001497795401138015,\n",
              "  0.0014834028673733123,\n",
              "  0.0015038402882161324,\n",
              "  0.0015308673298412938,\n",
              "  0.001549689480220846,\n",
              "  0.0015672716924389474,\n",
              "  0.001557946403766677,\n",
              "  0.0016077928018778701,\n",
              "  0.0016260651301627641,\n",
              "  0.0016417778154626534,\n",
              "  0.0016500607396724794,\n",
              "  0.001639405556457597,\n",
              "  0.001632290717441654,\n",
              "  0.0016157022036555199,\n",
              "  0.0016345790321495267,\n",
              "  0.0016052333161557263,\n",
              "  0.0016117443574286076,\n",
              "  0.0015979731239550972,\n",
              "  0.0016222992993233825,\n",
              "  0.0016125452873714838,\n",
              "  0.0016128298730207287,\n",
              "  0.0015991813326140711,\n",
              "  0.0016395940737129655,\n",
              "  0.0016579941818384454,\n",
              "  0.0016604099545369422,\n",
              "  0.0016052895902555154,\n",
              "  0.0016283366492837624,\n",
              "  0.0016105426243585995,\n",
              "  0.0016031798993733778],\n",
              " 'binary_logloss-mean': [0.6890079736941394,\n",
              "  0.6862051572772816,\n",
              "  0.6836323359176514,\n",
              "  0.6813924395681153,\n",
              "  0.6792049245266952,\n",
              "  0.677107823678238,\n",
              "  0.6752144171031245,\n",
              "  0.6737395874048735,\n",
              "  0.6720929846438661,\n",
              "  0.6709002204667353,\n",
              "  0.6694414620683545,\n",
              "  0.6683552465637308,\n",
              "  0.6673139033064118,\n",
              "  0.6663327957927284,\n",
              "  0.665112455319486,\n",
              "  0.6639166206219923,\n",
              "  0.6630925856330802,\n",
              "  0.6622629538612222,\n",
              "  0.6614108187708834,\n",
              "  0.6604194058200714,\n",
              "  0.6597802981568218,\n",
              "  0.6589704373579376,\n",
              "  0.65828468041011,\n",
              "  0.6574961516696719,\n",
              "  0.6567701451043305,\n",
              "  0.6560673133879273,\n",
              "  0.6553812591245297,\n",
              "  0.6547230055220564,\n",
              "  0.6542217886928984,\n",
              "  0.6536331016064567,\n",
              "  0.6531435116652642,\n",
              "  0.6526987444364488,\n",
              "  0.6521516676287392,\n",
              "  0.6516338091623531,\n",
              "  0.6512420113573014,\n",
              "  0.6507543955865965,\n",
              "  0.6503329299150651,\n",
              "  0.6498803128501036,\n",
              "  0.6493574778495989,\n",
              "  0.649030030891733,\n",
              "  0.6485846297864464,\n",
              "  0.6481487221536261,\n",
              "  0.64779211667322,\n",
              "  0.6473774094093779,\n",
              "  0.6469558535895231,\n",
              "  0.6465151073780323,\n",
              "  0.6461566813896087,\n",
              "  0.645853853836727,\n",
              "  0.6454869162411971,\n",
              "  0.6451888111202713,\n",
              "  0.6448669852832349,\n",
              "  0.6444835300662048,\n",
              "  0.644169420058096,\n",
              "  0.643873304208692,\n",
              "  0.6436251944692887,\n",
              "  0.6432749216761436,\n",
              "  0.6430213925818903,\n",
              "  0.6427927170149255,\n",
              "  0.642570641277642,\n",
              "  0.6423833856173456,\n",
              "  0.6421028360114214,\n",
              "  0.6418215764092219,\n",
              "  0.6416224494023351,\n",
              "  0.6414007463341408,\n",
              "  0.6412009904114897,\n",
              "  0.6410375235270792,\n",
              "  0.6407182126636616,\n",
              "  0.6405644052298562,\n",
              "  0.640321084551609,\n",
              "  0.6400722214135857,\n",
              "  0.6398207500223178,\n",
              "  0.639590975187685,\n",
              "  0.6393857252174123,\n",
              "  0.639120859649441,\n",
              "  0.6389448917688464,\n",
              "  0.6387054112092906,\n",
              "  0.6384677965624966,\n",
              "  0.6383302729636893,\n",
              "  0.6382276640978461,\n",
              "  0.6380344551844036,\n",
              "  0.6378678490733135,\n",
              "  0.6376573951355703,\n",
              "  0.6375040325911282,\n",
              "  0.6373431208592324,\n",
              "  0.6372970558894715,\n",
              "  0.6371476486812583,\n",
              "  0.636995260962861,\n",
              "  0.637069901286217,\n",
              "  0.6369521477876466,\n",
              "  0.6368572520171385,\n",
              "  0.6367146743565008,\n",
              "  0.6365816176199833,\n",
              "  0.6364307835554647,\n",
              "  0.6362984407942147,\n",
              "  0.6362508643706835,\n",
              "  0.6361291198184805,\n",
              "  0.6359927505361327,\n",
              "  0.6359229471878857,\n",
              "  0.6358640991036162,\n",
              "  0.6357605559136573,\n",
              "  0.635662113995113,\n",
              "  0.6355019793535225,\n",
              "  0.6354201020843137,\n",
              "  0.6352792766884297,\n",
              "  0.6351847327378678,\n",
              "  0.6350489894191508,\n",
              "  0.6348886574608043,\n",
              "  0.6347863872963612,\n",
              "  0.6346708653417305,\n",
              "  0.6345835999970909,\n",
              "  0.6344628606206154,\n",
              "  0.6343555163620872,\n",
              "  0.6342639710171398,\n",
              "  0.634187615220292,\n",
              "  0.6341369737974413,\n",
              "  0.6340024981136723,\n",
              "  0.6339163377543191,\n",
              "  0.6338567729387341,\n",
              "  0.633740493746552,\n",
              "  0.6339689183065733,\n",
              "  0.6336103273392342,\n",
              "  0.6335563655654238,\n",
              "  0.6336935637578679,\n",
              "  0.6336239807530588,\n",
              "  0.6335417737089514,\n",
              "  0.6334562948556074,\n",
              "  0.6333732324794781,\n",
              "  0.6332774792015283,\n",
              "  0.6330102339199867,\n",
              "  0.632906454012811,\n",
              "  0.6330744364316936,\n",
              "  0.6330068714611105,\n",
              "  0.6329422557528893,\n",
              "  0.6328620506068128,\n",
              "  0.632747201712288,\n",
              "  0.6326731552082719,\n",
              "  0.6326403970286583,\n",
              "  0.6326026031999626,\n",
              "  0.6325886337081278,\n",
              "  0.6325781210862268,\n",
              "  0.6325128129778824,\n",
              "  0.632450567061117,\n",
              "  0.6323991746202303,\n",
              "  0.6323774684419446,\n",
              "  0.632322497402264,\n",
              "  0.6324849118976656,\n",
              "  0.6324500200260625,\n",
              "  0.6323470726161305,\n",
              "  0.632276343407344,\n",
              "  0.6321896130917314,\n",
              "  0.632071258528787,\n",
              "  0.6319687658677351,\n",
              "  0.6319040203040608,\n",
              "  0.6318367320740238,\n",
              "  0.6317460632825123,\n",
              "  0.6317097615231355,\n",
              "  0.6316789427488229,\n",
              "  0.6316819217279628,\n",
              "  0.6316341681034956,\n",
              "  0.6316073274350007,\n",
              "  0.6315657084464675,\n",
              "  0.6315119277468152,\n",
              "  0.6315023723584581,\n",
              "  0.6314772336324735,\n",
              "  0.631389386338011,\n",
              "  0.6313600356731445,\n",
              "  0.6313318528797214,\n",
              "  0.6313074211039434,\n",
              "  0.6312811682253626,\n",
              "  0.6312576915758238,\n",
              "  0.6312080923295847,\n",
              "  0.6311716015031562,\n",
              "  0.6311115196559361,\n",
              "  0.6310486176009001,\n",
              "  0.6310180721047058,\n",
              "  0.6309343295758421,\n",
              "  0.6308697725422826,\n",
              "  0.630818940136966,\n",
              "  0.6307669451238875,\n",
              "  0.6307298213036967,\n",
              "  0.6306952809743015,\n",
              "  0.6306556692194686,\n",
              "  0.6306106081934173,\n",
              "  0.6305389676269522,\n",
              "  0.6305171952090273,\n",
              "  0.6304733922072022,\n",
              "  0.6304460649401333,\n",
              "  0.6304330938309618,\n",
              "  0.6303536707062347,\n",
              "  0.6303521867675667,\n",
              "  0.6303191386154786,\n",
              "  0.6302835044352603,\n",
              "  0.6302357437899619,\n",
              "  0.6301987072123619,\n",
              "  0.630192911419804,\n",
              "  0.6301335369680365,\n",
              "  0.630083976932066,\n",
              "  0.630045268102767,\n",
              "  0.6300208578997615,\n",
              "  0.6300019392295949,\n",
              "  0.6299947431477999,\n",
              "  0.6299209174726489,\n",
              "  0.6299197248987155,\n",
              "  0.629879120292817,\n",
              "  0.6298557612816623,\n",
              "  0.6298419173736463,\n",
              "  0.6297744786271688,\n",
              "  0.6297060647932967,\n",
              "  0.6296877540966018,\n",
              "  0.629660934101373,\n",
              "  0.6295985644678063,\n",
              "  0.629519291592997,\n",
              "  0.6295294450479938,\n",
              "  0.6295102923799502,\n",
              "  0.6294799538380733,\n",
              "  0.6294194960196101,\n",
              "  0.6293903474715649,\n",
              "  0.6293974350829076,\n",
              "  0.6293688982859315,\n",
              "  0.6293787377833896,\n",
              "  0.6293338888463467,\n",
              "  0.6293323752102341,\n",
              "  0.6292866529819822,\n",
              "  0.6292858259450816,\n",
              "  0.6295204284576766,\n",
              "  0.6291681708430735,\n",
              "  0.6292748604991362,\n",
              "  0.6292887351154239,\n",
              "  0.6294775677002513,\n",
              "  0.6291872791860355,\n",
              "  0.6293875924830453,\n",
              "  0.6293774757530535,\n",
              "  0.6293823652901241,\n",
              "  0.6293125224752355,\n",
              "  0.629025639071637,\n",
              "  0.6290211396321758,\n",
              "  0.6286568082962535,\n",
              "  0.6286269358223571,\n",
              "  0.6286102272650114,\n",
              "  0.6286015677440095,\n",
              "  0.6286082665528467,\n",
              "  0.628594285028568,\n",
              "  0.6288810107325185,\n",
              "  0.6288236845495531,\n",
              "  0.6288112402600654,\n",
              "  0.6287654485326585,\n",
              "  0.6287700625261619,\n",
              "  0.6287449289490389,\n",
              "  0.628714789538287,\n",
              "  0.6287240183351736,\n",
              "  0.6287095820196629,\n",
              "  0.6286787655224447,\n",
              "  0.6286743552767317,\n",
              "  0.6289385690835559,\n",
              "  0.6289356920218848,\n",
              "  0.6289263881811334,\n",
              "  0.6289092096127341,\n",
              "  0.6288993227169088,\n",
              "  0.6291136378477488,\n",
              "  0.6290489364499179,\n",
              "  0.6290420377667386,\n",
              "  0.6289993618180304,\n",
              "  0.6289815739071324,\n",
              "  0.6289479279933371,\n",
              "  0.6289402908644998,\n",
              "  0.6289354410367869,\n",
              "  0.6289048587780234,\n",
              "  0.628893917916617,\n",
              "  0.6288794578849831,\n",
              "  0.6288461110413269,\n",
              "  0.6288021885035772,\n",
              "  0.6287825021687401,\n",
              "  0.6287823081239098,\n",
              "  0.6287643487571053,\n",
              "  0.6287325142220831,\n",
              "  0.6286815300212698,\n",
              "  0.6286624634925605,\n",
              "  0.6286601774987245,\n",
              "  0.6286483967261857,\n",
              "  0.6286449430066225,\n",
              "  0.6286055731303702,\n",
              "  0.6285973585227759,\n",
              "  0.6286213369297794,\n",
              "  0.6283215058418387,\n",
              "  0.6282992810218856,\n",
              "  0.6288258661152784,\n",
              "  0.6288244222737203,\n",
              "  0.6290759997384895,\n",
              "  0.6290884489882197,\n",
              "  0.6287896899748922,\n",
              "  0.6287510114192097,\n",
              "  0.6287561518085752,\n",
              "  0.6287281764012443,\n",
              "  0.6287253407921249,\n",
              "  0.6287116366823092,\n",
              "  0.6286883493973265,\n",
              "  0.6286612047941408,\n",
              "  0.6286305640464502,\n",
              "  0.628620507049985,\n",
              "  0.6286217340343455,\n",
              "  0.6286324853702426,\n",
              "  0.6286393709497725,\n",
              "  0.6286418352149844,\n",
              "  0.6286072696704957,\n",
              "  0.6283631831544886,\n",
              "  0.6283571629069596,\n",
              "  0.6283621188397464,\n",
              "  0.6285804024157636,\n",
              "  0.628555678787655,\n",
              "  0.6285564085871199,\n",
              "  0.6285024283857167,\n",
              "  0.6284662199438417,\n",
              "  0.6284552389429284,\n",
              "  0.6283710988716716,\n",
              "  0.6283740137203556,\n",
              "  0.6283483604542471,\n",
              "  0.628326361559843,\n",
              "  0.6283244395098245,\n",
              "  0.6283268538085036,\n",
              "  0.6285648688346613,\n",
              "  0.628248623485602,\n",
              "  0.628464544424233,\n",
              "  0.6284598587160358,\n",
              "  0.6284347561419317,\n",
              "  0.628391394611624,\n",
              "  0.6283825698023864,\n",
              "  0.6283928394836176,\n",
              "  0.628384193405697,\n",
              "  0.6283833910717297,\n",
              "  0.6283922464327402,\n",
              "  0.6283788749624248,\n",
              "  0.6283491332341763,\n",
              "  0.6283611134793633,\n",
              "  0.6283633494385157,\n",
              "  0.6283541023817726,\n",
              "  0.6283334821456453,\n",
              "  0.6283016990543887,\n",
              "  0.6282986304016887,\n",
              "  0.6282836514833882,\n",
              "  0.6282542144295895,\n",
              "  0.6282320276907555,\n",
              "  0.6282448199493161,\n",
              "  0.6282463292541175,\n",
              "  0.6282543544275176,\n",
              "  0.6279765686334678,\n",
              "  0.6279921115871666,\n",
              "  0.6280047344406358,\n",
              "  0.6279761308542425,\n",
              "  0.6279766904341191,\n",
              "  0.6279646955434318,\n",
              "  0.6279641332200371,\n",
              "  0.6279670366319642,\n",
              "  0.6279595319587724,\n",
              "  0.6279511330078924,\n",
              "  0.6279390213621738,\n",
              "  0.627644475261213,\n",
              "  0.6276260475609443],\n",
              " 'binary_logloss-stdv': [1.8362574777308138e-05,\n",
              "  3.754562149089942e-05,\n",
              "  3.6717011325671806e-05,\n",
              "  1.9285043363184458e-05,\n",
              "  4.268714820919677e-06,\n",
              "  8.223549478006447e-05,\n",
              "  0.00016387875673774642,\n",
              "  0.00019447393244335296,\n",
              "  0.000188181829391129,\n",
              "  0.00016714197162498052,\n",
              "  0.00020749862986847029,\n",
              "  0.00020878002853485467,\n",
              "  0.00029211976864551543,\n",
              "  0.0003297963719324975,\n",
              "  0.000379818341378636,\n",
              "  0.0004076210082658835,\n",
              "  0.0004361385849044775,\n",
              "  0.0003518036706884293,\n",
              "  0.0004206406234821218,\n",
              "  0.00040244158378318495,\n",
              "  0.0003831265165545846,\n",
              "  0.00028642760173244915,\n",
              "  0.00034412455934210444,\n",
              "  0.0003330449993213308,\n",
              "  0.00025124226032134525,\n",
              "  0.00020252379305729307,\n",
              "  0.0002365039680975141,\n",
              "  0.00029491530247610084,\n",
              "  0.0002803020108039255,\n",
              "  0.0002858632277428333,\n",
              "  0.0002455601489531301,\n",
              "  0.00022743407845350413,\n",
              "  0.00029840944778177577,\n",
              "  0.00036336511696649675,\n",
              "  0.0003450077744755995,\n",
              "  0.0003192705365967339,\n",
              "  0.00033390703510866406,\n",
              "  0.00033739611217557396,\n",
              "  0.00030975393074745373,\n",
              "  0.00030822688984003644,\n",
              "  0.00036286063933992985,\n",
              "  0.0003034604120601172,\n",
              "  0.000358480728904746,\n",
              "  0.0003322847042253674,\n",
              "  0.0003938307449579896,\n",
              "  0.0004578986759906925,\n",
              "  0.0005112766167114423,\n",
              "  0.0005377144026336208,\n",
              "  0.0005616764811029477,\n",
              "  0.0005615631310380432,\n",
              "  0.0006246690932142885,\n",
              "  0.0006676661536807304,\n",
              "  0.0007036592113574051,\n",
              "  0.0006793503156205841,\n",
              "  0.0006760384820362402,\n",
              "  0.0006832015829212194,\n",
              "  0.0006705834161846993,\n",
              "  0.0006733937237160301,\n",
              "  0.0005940535246902695,\n",
              "  0.0005647578083320438,\n",
              "  0.0005903229902635088,\n",
              "  0.0005819259088741371,\n",
              "  0.0005746035465989057,\n",
              "  0.0005177865025428925,\n",
              "  0.0005212764266163539,\n",
              "  0.00046916349220386873,\n",
              "  0.0004949457424853281,\n",
              "  0.000560437208111881,\n",
              "  0.0005817287301478908,\n",
              "  0.0005942885831093272,\n",
              "  0.0006071465739178145,\n",
              "  0.0006126823880983598,\n",
              "  0.0006435936973660478,\n",
              "  0.0006299997029516154,\n",
              "  0.0006239198912910992,\n",
              "  0.0005921851366849306,\n",
              "  0.0006291311440835218,\n",
              "  0.0005670936889414312,\n",
              "  0.0005614290672133142,\n",
              "  0.000610740062415011,\n",
              "  0.0005968640639097616,\n",
              "  0.0006267940017261439,\n",
              "  0.0006130932300904226,\n",
              "  0.0006191538125274343,\n",
              "  0.0006377673350274402,\n",
              "  0.0006364583243270706,\n",
              "  0.0006260538837983221,\n",
              "  0.0004547052964901069,\n",
              "  0.00044916358868884743,\n",
              "  0.0004789139684828805,\n",
              "  0.0004949679270293268,\n",
              "  0.0004835086337242777,\n",
              "  0.0005081429488032485,\n",
              "  0.0005061348919530075,\n",
              "  0.0005225876854569483,\n",
              "  0.000548525887238197,\n",
              "  0.0006149493364162448,\n",
              "  0.0006280968090280892,\n",
              "  0.0005706738657948082,\n",
              "  0.0005837178086433966,\n",
              "  0.0005656022422378403,\n",
              "  0.000573870339531376,\n",
              "  0.0005687759726256267,\n",
              "  0.0005735219293291025,\n",
              "  0.0005216848180584081,\n",
              "  0.000507325459815173,\n",
              "  0.000530158563890321,\n",
              "  0.0005216791191516289,\n",
              "  0.0005272858280288761,\n",
              "  0.0005376745102569732,\n",
              "  0.0005303476839282283,\n",
              "  0.000575090545704966,\n",
              "  0.0006139503759405998,\n",
              "  0.0006089375358573237,\n",
              "  0.0006214589229906693,\n",
              "  0.0006333429013515557,\n",
              "  0.000596937355119962,\n",
              "  0.0005881185239835684,\n",
              "  0.0005491628614703649,\n",
              "  0.0008862424289817492,\n",
              "  0.0005459811501488216,\n",
              "  0.0005868515427048622,\n",
              "  0.00087772060257033,\n",
              "  0.000890205829275275,\n",
              "  0.0008571246041311297,\n",
              "  0.0008909785365446895,\n",
              "  0.0008869339109296923,\n",
              "  0.0008852093998108363,\n",
              "  0.0006016282075120709,\n",
              "  0.000588972316029885,\n",
              "  0.0009042503817026622,\n",
              "  0.0009173026231797068,\n",
              "  0.0009546092848660594,\n",
              "  0.000958705330323656,\n",
              "  0.00097790509206801,\n",
              "  0.0009635130035878787,\n",
              "  0.0009285429930709652,\n",
              "  0.0009289074996831134,\n",
              "  0.0009080359826440516,\n",
              "  0.000906318448276852,\n",
              "  0.0008768586628855264,\n",
              "  0.0008595426409410799,\n",
              "  0.0008698750197025392,\n",
              "  0.0008406216411483345,\n",
              "  0.0008303538331526127,\n",
              "  0.0007530070409491999,\n",
              "  0.0007553233856103916,\n",
              "  0.0007858332665361296,\n",
              "  0.0007970259730091844,\n",
              "  0.0008057765992760179,\n",
              "  0.0007955431584968189,\n",
              "  0.0008534673366817745,\n",
              "  0.0008848136490229788,\n",
              "  0.0009368856053780997,\n",
              "  0.0009966451944924512,\n",
              "  0.0009822272539614207,\n",
              "  0.0009829174285029083,\n",
              "  0.0009853796082844373,\n",
              "  0.0009679318710050437,\n",
              "  0.0009513090681804821,\n",
              "  0.000980457069987647,\n",
              "  0.000970173906685083,\n",
              "  0.0009845681921700656,\n",
              "  0.0009665512073241994,\n",
              "  0.0009494497038508651,\n",
              "  0.000974989777535424,\n",
              "  0.0009850914301075602,\n",
              "  0.0009744514460525422,\n",
              "  0.0009244321965695342,\n",
              "  0.0009203278127933248,\n",
              "  0.0009129330367816445,\n",
              "  0.0009050279548867025,\n",
              "  0.0008754192624614801,\n",
              "  0.0008163118042063937,\n",
              "  0.0008170227532014779,\n",
              "  0.0008152995870430715,\n",
              "  0.0008173663825831906,\n",
              "  0.0008095575136007146,\n",
              "  0.0007900228856830206,\n",
              "  0.0007771376852424482,\n",
              "  0.0007705465746121065,\n",
              "  0.0007528042633221461,\n",
              "  0.0007613707455997933,\n",
              "  0.0007218497724400592,\n",
              "  0.000750687326552923,\n",
              "  0.0007766871494673912,\n",
              "  0.0007546767385385838,\n",
              "  0.0007556485621053718,\n",
              "  0.0007725969078978632,\n",
              "  0.0007491950984534996,\n",
              "  0.0007066934486315567,\n",
              "  0.0007013826555240625,\n",
              "  0.0006865282190395271,\n",
              "  0.0006460872547451835,\n",
              "  0.0006329234704525956,\n",
              "  0.0006504420827691587,\n",
              "  0.0006502505249760365,\n",
              "  0.0006598770921201153,\n",
              "  0.0006577134141222032,\n",
              "  0.0006471420137925179,\n",
              "  0.0006074718691351277,\n",
              "  0.0005518157506415528,\n",
              "  0.0005235312205970243,\n",
              "  0.0005630127610470923,\n",
              "  0.0005540492275574712,\n",
              "  0.0005592904267603033,\n",
              "  0.0005589538620578288,\n",
              "  0.0005825644365554437,\n",
              "  0.0005894212579971834,\n",
              "  0.0005954287210227654,\n",
              "  0.0005381356931297206,\n",
              "  0.0004904279571877553,\n",
              "  0.0004892335541729957,\n",
              "  0.000487794559388923,\n",
              "  0.00046536160474870854,\n",
              "  0.0004760790587592022,\n",
              "  0.000505037237922227,\n",
              "  0.0005024557038973159,\n",
              "  0.000506043763919001,\n",
              "  0.0004852750490778877,\n",
              "  0.00046780611242480295,\n",
              "  0.00045620452436151806,\n",
              "  0.00044603668800448094,\n",
              "  0.0004008056442950242,\n",
              "  0.0003565124162659105,\n",
              "  0.000392644536397304,\n",
              "  0.0004529269641676977,\n",
              "  0.00048861364970552,\n",
              "  0.0007214765472236594,\n",
              "  0.0005043179058531946,\n",
              "  0.0007056678550186015,\n",
              "  0.0007126947897946182,\n",
              "  0.0007109108369159115,\n",
              "  0.0006928175916827094,\n",
              "  0.00044092366263760686,\n",
              "  0.0004514605099558653,\n",
              "  0.0004330893392462977,\n",
              "  0.0004364934451246739,\n",
              "  0.0004509501431219008,\n",
              "  0.00047559598512683014,\n",
              "  0.0004607495349085295,\n",
              "  0.0004532618370683715,\n",
              "  0.00045882430623391766,\n",
              "  0.00047530441827994943,\n",
              "  0.0004606987720348655,\n",
              "  0.00042009397312031747,\n",
              "  0.00040746498271208365,\n",
              "  0.0003935806459677473,\n",
              "  0.00037639023758978356,\n",
              "  0.00037225450603326995,\n",
              "  0.00037597852303613595,\n",
              "  0.00041321369675897517,\n",
              "  0.00043109655314568703,\n",
              "  0.00017856047630189403,\n",
              "  0.00017773260440371571,\n",
              "  0.00016867040613869597,\n",
              "  0.00013578745984504033,\n",
              "  0.00015631799659143836,\n",
              "  0.0004801951982524785,\n",
              "  0.0004940819283082041,\n",
              "  0.0005190457489787995,\n",
              "  0.0005219134786229173,\n",
              "  0.0005301347641667846,\n",
              "  0.0005182717920115328,\n",
              "  0.0004976775436431118,\n",
              "  0.0005121886834161996,\n",
              "  0.00045383280605492245,\n",
              "  0.00041750904046132493,\n",
              "  0.0004073969396512897,\n",
              "  0.0003674078777428767,\n",
              "  0.00038489198887137834,\n",
              "  0.00037971917303752,\n",
              "  0.0003802887387357306,\n",
              "  0.0003818454046693717,\n",
              "  0.0003929278656034347,\n",
              "  0.0003990175334568753,\n",
              "  0.00038719127281275627,\n",
              "  0.0004030790677896896,\n",
              "  0.0004073775494476829,\n",
              "  0.00040643600873128044,\n",
              "  0.0004018828957168029,\n",
              "  0.0004044640011899166,\n",
              "  0.0004203516565296618,\n",
              "  0.0006645789221028808,\n",
              "  0.0006493571106755911,\n",
              "  0.0004625835237345018,\n",
              "  0.0004637598333500958,\n",
              "  0.0007469254138008427,\n",
              "  0.0007422732284459917,\n",
              "  0.0004407017768000803,\n",
              "  0.00042959403650012155,\n",
              "  0.0004484741232911182,\n",
              "  0.0004421281564322119,\n",
              "  0.000434213884278931,\n",
              "  0.00043980516223845984,\n",
              "  0.0004394279400900406,\n",
              "  0.0004256236149344467,\n",
              "  0.00043638895388126565,\n",
              "  0.0004283514644142244,\n",
              "  0.0004126311997333023,\n",
              "  0.0004134973411226944,\n",
              "  0.00042067339390978954,\n",
              "  0.0004267667430963828,\n",
              "  0.0004360026540969616,\n",
              "  0.0008012844150293181,\n",
              "  0.0007954292222912161,\n",
              "  0.0007892009010507149,\n",
              "  0.0010449117990481707,\n",
              "  0.0010732573768969657,\n",
              "  0.0010471851044143985,\n",
              "  0.0010957672838652168,\n",
              "  0.0011078275407976849,\n",
              "  0.0010951492798657707,\n",
              "  0.0011257497556836735,\n",
              "  0.001132407027031166,\n",
              "  0.0011403529596953682,\n",
              "  0.001123027547986574,\n",
              "  0.0011265012043389683,\n",
              "  0.0011283876149663801,\n",
              "  0.0014199191619459763,\n",
              "  0.0011274488325835616,\n",
              "  0.0014360572229841608,\n",
              "  0.0014429908122192223,\n",
              "  0.0014424787195075498,\n",
              "  0.0014726557528529764,\n",
              "  0.0014829195423621217,\n",
              "  0.0014724758461820882,\n",
              "  0.0014871829744428416,\n",
              "  0.0015120653750524005,\n",
              "  0.001501077355963183,\n",
              "  0.001502541517693723,\n",
              "  0.0015131194895528738,\n",
              "  0.0015185120665568643,\n",
              "  0.0015244638673867046,\n",
              "  0.0015034922041820872,\n",
              "  0.0015266529036961343,\n",
              "  0.0015290969491219856,\n",
              "  0.0015390445273910042,\n",
              "  0.0015367578421596774,\n",
              "  0.001529544974742031,\n",
              "  0.0015313896780109163,\n",
              "  0.0015185425106809384,\n",
              "  0.0015273615099885454,\n",
              "  0.0015120865731053901,\n",
              "  0.0012142814331006274,\n",
              "  0.001204075345970521,\n",
              "  0.0012088037354816242,\n",
              "  0.0012108130980307652,\n",
              "  0.0012153803256053288,\n",
              "  0.0012113438409850044,\n",
              "  0.0012262072039159195,\n",
              "  0.0012335173869306236,\n",
              "  0.0012408856723497533,\n",
              "  0.0012070841121892453,\n",
              "  0.0012184992415783772,\n",
              "  0.0011890664780415368,\n",
              "  0.0011725999229429468],\n",
              " 'cross_entropy-mean': [0.6890079736941394,\n",
              "  0.6862051572772816,\n",
              "  0.6836323359176514,\n",
              "  0.6813924395681153,\n",
              "  0.6792049245266952,\n",
              "  0.677107823678238,\n",
              "  0.6752144171031245,\n",
              "  0.6737395874048735,\n",
              "  0.6720929846438661,\n",
              "  0.6709002204667353,\n",
              "  0.6694414620683545,\n",
              "  0.6683552465637308,\n",
              "  0.6673139033064118,\n",
              "  0.6663327957927284,\n",
              "  0.665112455319486,\n",
              "  0.6639166206219923,\n",
              "  0.6630925856330802,\n",
              "  0.6622629538612222,\n",
              "  0.6614108187708834,\n",
              "  0.6604194058200714,\n",
              "  0.6597802981568218,\n",
              "  0.6589704373579376,\n",
              "  0.65828468041011,\n",
              "  0.6574961516696719,\n",
              "  0.6567701451043305,\n",
              "  0.6560673133879273,\n",
              "  0.6553812591245297,\n",
              "  0.6547230055220564,\n",
              "  0.6542217886928984,\n",
              "  0.6536331016064567,\n",
              "  0.6531435116652642,\n",
              "  0.6526987444364488,\n",
              "  0.6521516676287392,\n",
              "  0.6516338091623531,\n",
              "  0.6512420113573014,\n",
              "  0.6507543955865965,\n",
              "  0.6503329299150651,\n",
              "  0.6498803128501036,\n",
              "  0.6493574778495989,\n",
              "  0.649030030891733,\n",
              "  0.6485846297864464,\n",
              "  0.6481487221536261,\n",
              "  0.64779211667322,\n",
              "  0.6473774094093779,\n",
              "  0.6469558535895231,\n",
              "  0.6465151073780323,\n",
              "  0.6461566813896087,\n",
              "  0.645853853836727,\n",
              "  0.6454869162411971,\n",
              "  0.6451888111202713,\n",
              "  0.6448669852832349,\n",
              "  0.6444835300662048,\n",
              "  0.644169420058096,\n",
              "  0.643873304208692,\n",
              "  0.6436251944692887,\n",
              "  0.6432749216761436,\n",
              "  0.6430213925818903,\n",
              "  0.6427927170149255,\n",
              "  0.642570641277642,\n",
              "  0.6423833856173456,\n",
              "  0.6421028360114214,\n",
              "  0.6418215764092219,\n",
              "  0.6416224494023351,\n",
              "  0.6414007463341408,\n",
              "  0.6412009904114897,\n",
              "  0.6410375235270792,\n",
              "  0.6407182126636616,\n",
              "  0.6405644052298562,\n",
              "  0.640321084551609,\n",
              "  0.6400722214135857,\n",
              "  0.6398207500223178,\n",
              "  0.639590975187685,\n",
              "  0.6393857252174123,\n",
              "  0.639120859649441,\n",
              "  0.6389448917688464,\n",
              "  0.6387054112092906,\n",
              "  0.6384677965624966,\n",
              "  0.6383302729636893,\n",
              "  0.6382276640978461,\n",
              "  0.6380344551844036,\n",
              "  0.6378678490733135,\n",
              "  0.6376573951355703,\n",
              "  0.6375040325911282,\n",
              "  0.6373431208592324,\n",
              "  0.6372970558894715,\n",
              "  0.6371476486812583,\n",
              "  0.636995260962861,\n",
              "  0.6370041131406762,\n",
              "  0.6368863596421059,\n",
              "  0.6367914638715977,\n",
              "  0.63664888621096,\n",
              "  0.6365158294744427,\n",
              "  0.6363649954099239,\n",
              "  0.636232652648674,\n",
              "  0.6361850762251428,\n",
              "  0.6360633316729398,\n",
              "  0.635926962390592,\n",
              "  0.6358571590423449,\n",
              "  0.6357983109580755,\n",
              "  0.6356947677681166,\n",
              "  0.6355963258495722,\n",
              "  0.6354361912079818,\n",
              "  0.6353543139387728,\n",
              "  0.635213488542889,\n",
              "  0.635118944592327,\n",
              "  0.63498320127361,\n",
              "  0.6348228693152634,\n",
              "  0.6347205991508205,\n",
              "  0.6346050771961896,\n",
              "  0.6345178118515502,\n",
              "  0.6343970724750747,\n",
              "  0.6342897282165465,\n",
              "  0.6341981828715991,\n",
              "  0.6341218270747513,\n",
              "  0.6340711856519006,\n",
              "  0.6339367099681316,\n",
              "  0.6338505496087784,\n",
              "  0.6337909847931934,\n",
              "  0.6336747056010111,\n",
              "  0.6338373420154918,\n",
              "  0.6335445391936935,\n",
              "  0.6334905774198831,\n",
              "  0.6335619874667864,\n",
              "  0.6334924044619774,\n",
              "  0.63341019741787,\n",
              "  0.6333247185645261,\n",
              "  0.6332416561883966,\n",
              "  0.633145902910447,\n",
              "  0.632944445774446,\n",
              "  0.6328406658672703,\n",
              "  0.6329428601406123,\n",
              "  0.632875295170029,\n",
              "  0.6328106794618079,\n",
              "  0.6327304743157313,\n",
              "  0.6326156254212065,\n",
              "  0.6325415789171904,\n",
              "  0.6325088207375767,\n",
              "  0.6324710269088812,\n",
              "  0.6324570574170463,\n",
              "  0.6324465447951453,\n",
              "  0.632381236686801,\n",
              "  0.6323189907700354,\n",
              "  0.632267598329149,\n",
              "  0.6322458921508632,\n",
              "  0.6321909211111826,\n",
              "  0.6322875474610434,\n",
              "  0.6322526555894403,\n",
              "  0.6321497081795083,\n",
              "  0.6320789789707218,\n",
              "  0.6319922486551094,\n",
              "  0.6318738940921648,\n",
              "  0.6317714014311128,\n",
              "  0.6317066558674387,\n",
              "  0.6316393676374017,\n",
              "  0.6315486988458899,\n",
              "  0.6315123970865134,\n",
              "  0.6314815783122006,\n",
              "  0.6314845572913406,\n",
              "  0.6314368036668735,\n",
              "  0.6314099629983786,\n",
              "  0.6313683440098453,\n",
              "  0.631314563310193,\n",
              "  0.631305007921836,\n",
              "  0.6312798691958513,\n",
              "  0.6311920219013888,\n",
              "  0.6311626712365223,\n",
              "  0.6311344884430993,\n",
              "  0.6311100566673212,\n",
              "  0.6310838037887404,\n",
              "  0.6310603271392016,\n",
              "  0.6310107278929625,\n",
              "  0.6309742370665341,\n",
              "  0.6309141552193139,\n",
              "  0.6308512531642781,\n",
              "  0.6308207076680836,\n",
              "  0.6307369651392198,\n",
              "  0.6306724081056604,\n",
              "  0.6306215757003436,\n",
              "  0.6305695806872653,\n",
              "  0.6305324568670746,\n",
              "  0.6304979165376793,\n",
              "  0.6304583047828464,\n",
              "  0.6304132437567952,\n",
              "  0.63034160319033,\n",
              "  0.630319830772405,\n",
              "  0.63027602777058,\n",
              "  0.6302487005035111,\n",
              "  0.6302357293943396,\n",
              "  0.6301563062696126,\n",
              "  0.6301548223309444,\n",
              "  0.6301217741788565,\n",
              "  0.6300861399986382,\n",
              "  0.6300383793533397,\n",
              "  0.6300013427757398,\n",
              "  0.6299955469831818,\n",
              "  0.6299361725314144,\n",
              "  0.6298866124954438,\n",
              "  0.6298479036661446,\n",
              "  0.6298234934631394,\n",
              "  0.6298045747929727,\n",
              "  0.6297973787111776,\n",
              "  0.6297235530360267,\n",
              "  0.6297223604620933,\n",
              "  0.629681755856195,\n",
              "  0.6296583968450401,\n",
              "  0.629644552937024,\n",
              "  0.6295771141905466,\n",
              "  0.6295087003566745,\n",
              "  0.6294903896599796,\n",
              "  0.6294635696647508,\n",
              "  0.6294012000311843,\n",
              "  0.6293219271563748,\n",
              "  0.6293320806113716,\n",
              "  0.6293129279433279,\n",
              "  0.6292825894014511,\n",
              "  0.6292221315829879,\n",
              "  0.6291929830349426,\n",
              "  0.6292000706462856,\n",
              "  0.6291715338493092,\n",
              "  0.6291813733467676,\n",
              "  0.6291365244097245,\n",
              "  0.6291350107736119,\n",
              "  0.62908928854536,\n",
              "  0.6290884615084594,\n",
              "  0.6292572758755136,\n",
              "  0.6289708064064513,\n",
              "  0.6290117079169731,\n",
              "  0.629025582533261,\n",
              "  0.6291486269725476,\n",
              "  0.6289241266038726,\n",
              "  0.6290586517553417,\n",
              "  0.6290485350253499,\n",
              "  0.6290534245624205,\n",
              "  0.6289835817475318,\n",
              "  0.6287624864894741,\n",
              "  0.628757987050013,\n",
              "  0.6284594438596313,\n",
              "  0.628429571385735,\n",
              "  0.6284128628283892,\n",
              "  0.6284042033073873,\n",
              "  0.6284109021162246,\n",
              "  0.6283969205919457,\n",
              "  0.6286178581503555,\n",
              "  0.62856053196739,\n",
              "  0.6285480876779026,\n",
              "  0.6285022959504954,\n",
              "  0.628506909943999,\n",
              "  0.628481776366876,\n",
              "  0.628451636956124,\n",
              "  0.6284608657530106,\n",
              "  0.6284464294375,\n",
              "  0.6284156129402817,\n",
              "  0.6284112026945686,\n",
              "  0.6286096283558522,\n",
              "  0.6286067512941811,\n",
              "  0.6285974474534298,\n",
              "  0.6285802688850305,\n",
              "  0.6285703819892053,\n",
              "  0.6287189089745044,\n",
              "  0.6286542075766735,\n",
              "  0.6286473088934942,\n",
              "  0.628604632944786,\n",
              "  0.6285868450338882,\n",
              "  0.6285531991200929,\n",
              "  0.6285455619912553,\n",
              "  0.6285407121635426,\n",
              "  0.6285101299047791,\n",
              "  0.6284991890433727,\n",
              "  0.6284847290117389,\n",
              "  0.6284513821680826,\n",
              "  0.6284074596303327,\n",
              "  0.6283877732954957,\n",
              "  0.6283875792506654,\n",
              "  0.6283696198838608,\n",
              "  0.6283377853488389,\n",
              "  0.6282868011480255,\n",
              "  0.6282677346193161,\n",
              "  0.6282654486254802,\n",
              "  0.6282536678529412,\n",
              "  0.6282502141333782,\n",
              "  0.6282108442571259,\n",
              "  0.6282026296495314,\n",
              "  0.6282266080565351,\n",
              "  0.627992565114135,\n",
              "  0.627970340294182,\n",
              "  0.6283653490964932,\n",
              "  0.6283639052549352,\n",
              "  0.6285496945741638,\n",
              "  0.628562143823894,\n",
              "  0.628329172956107,\n",
              "  0.6282904944004247,\n",
              "  0.6282956347897901,\n",
              "  0.6282676593824593,\n",
              "  0.6282648237733398,\n",
              "  0.6282511196635242,\n",
              "  0.6282278323785415,\n",
              "  0.6282006877753559,\n",
              "  0.628170047027665,\n",
              "  0.6281599900311999,\n",
              "  0.6281612170155605,\n",
              "  0.6281719683514575,\n",
              "  0.6281788539309873,\n",
              "  0.6281813181961993,\n",
              "  0.6281467526517107,\n",
              "  0.6279684542812441,\n",
              "  0.6279624340337152,\n",
              "  0.6279673899665018,\n",
              "  0.6281198853969787,\n",
              "  0.6280951617688699,\n",
              "  0.6280958915683348,\n",
              "  0.6280419113669314,\n",
              "  0.6280057029250564,\n",
              "  0.6279947219241436,\n",
              "  0.6279105818528864,\n",
              "  0.6279134967015706,\n",
              "  0.6278878434354619,\n",
              "  0.627865844541058,\n",
              "  0.6278639224910395,\n",
              "  0.6278663367897185,\n",
              "  0.6280385636703355,\n",
              "  0.6277881064668169,\n",
              "  0.6279382392599073,\n",
              "  0.62793355355171,\n",
              "  0.627908450977606,\n",
              "  0.627865089447298,\n",
              "  0.6278562646380605,\n",
              "  0.6278665343192918,\n",
              "  0.6278578882413711,\n",
              "  0.6278570859074039,\n",
              "  0.6278659412684142,\n",
              "  0.6278525697980989,\n",
              "  0.6278228280698506,\n",
              "  0.6278348083150372,\n",
              "  0.6278370442741898,\n",
              "  0.6278277972174466,\n",
              "  0.6278071769813195,\n",
              "  0.6277753938900629,\n",
              "  0.6277723252373629,\n",
              "  0.6277573463190622,\n",
              "  0.6277279092652638,\n",
              "  0.6277057225264296,\n",
              "  0.6277185147849903,\n",
              "  0.6277200240897917,\n",
              "  0.6277280492631919,\n",
              "  0.6275160516146828,\n",
              "  0.6275315945683816,\n",
              "  0.6275442174218507,\n",
              "  0.6275156138354575,\n",
              "  0.6275161734153339,\n",
              "  0.6275041785246468,\n",
              "  0.627503616201252,\n",
              "  0.6275065196131792,\n",
              "  0.6274990149399874,\n",
              "  0.6274906159891073,\n",
              "  0.6274785043433887,\n",
              "  0.6272497463879687,\n",
              "  0.6272313186877],\n",
              " 'cross_entropy-stdv': [1.8362574777308138e-05,\n",
              "  3.754562149089942e-05,\n",
              "  3.6717011325671806e-05,\n",
              "  1.9285043363184458e-05,\n",
              "  4.268714820919677e-06,\n",
              "  8.223549478006447e-05,\n",
              "  0.00016387875673774642,\n",
              "  0.00019447393244335296,\n",
              "  0.000188181829391129,\n",
              "  0.00016714197162498052,\n",
              "  0.00020749862986847029,\n",
              "  0.00020878002853485467,\n",
              "  0.00029211976864551543,\n",
              "  0.0003297963719324975,\n",
              "  0.000379818341378636,\n",
              "  0.0004076210082658835,\n",
              "  0.0004361385849044775,\n",
              "  0.0003518036706884293,\n",
              "  0.0004206406234821218,\n",
              "  0.00040244158378318495,\n",
              "  0.0003831265165545846,\n",
              "  0.00028642760173244915,\n",
              "  0.00034412455934210444,\n",
              "  0.0003330449993213308,\n",
              "  0.00025124226032134525,\n",
              "  0.00020252379305729307,\n",
              "  0.0002365039680975141,\n",
              "  0.00029491530247610084,\n",
              "  0.0002803020108039255,\n",
              "  0.0002858632277428333,\n",
              "  0.0002455601489531301,\n",
              "  0.00022743407845350413,\n",
              "  0.00029840944778177577,\n",
              "  0.00036336511696649675,\n",
              "  0.0003450077744755995,\n",
              "  0.0003192705365967339,\n",
              "  0.00033390703510866406,\n",
              "  0.00033739611217557396,\n",
              "  0.00030975393074745373,\n",
              "  0.00030822688984003644,\n",
              "  0.00036286063933992985,\n",
              "  0.0003034604120601172,\n",
              "  0.000358480728904746,\n",
              "  0.0003322847042253674,\n",
              "  0.0003938307449579896,\n",
              "  0.0004578986759906925,\n",
              "  0.0005112766167114423,\n",
              "  0.0005377144026336208,\n",
              "  0.0005616764811029477,\n",
              "  0.0005615631310380432,\n",
              "  0.0006246690932142885,\n",
              "  0.0006676661536807304,\n",
              "  0.0007036592113574051,\n",
              "  0.0006793503156205841,\n",
              "  0.0006760384820362402,\n",
              "  0.0006832015829212194,\n",
              "  0.0006705834161846993,\n",
              "  0.0006733937237160301,\n",
              "  0.0005940535246902695,\n",
              "  0.0005647578083320438,\n",
              "  0.0005903229902635088,\n",
              "  0.0005819259088741371,\n",
              "  0.0005746035465989057,\n",
              "  0.0005177865025428925,\n",
              "  0.0005212764266163539,\n",
              "  0.00046916349220386873,\n",
              "  0.0004949457424853281,\n",
              "  0.000560437208111881,\n",
              "  0.0005817287301478908,\n",
              "  0.0005942885831093272,\n",
              "  0.0006071465739178145,\n",
              "  0.0006126823880983598,\n",
              "  0.0006435936973660478,\n",
              "  0.0006299997029516154,\n",
              "  0.0006239198912910992,\n",
              "  0.0005921851366849306,\n",
              "  0.0006291311440835218,\n",
              "  0.0005670936889414312,\n",
              "  0.0005614290672133142,\n",
              "  0.000610740062415011,\n",
              "  0.0005968640639097616,\n",
              "  0.0006267940017261439,\n",
              "  0.0006130932300904226,\n",
              "  0.0006191538125274343,\n",
              "  0.0006377673350274402,\n",
              "  0.0006364583243270706,\n",
              "  0.0006260538837983221,\n",
              "  0.00047409291095908075,\n",
              "  0.0004712204634949386,\n",
              "  0.0004945200653785899,\n",
              "  0.0005143440354762988,\n",
              "  0.0005049055581627052,\n",
              "  0.0005378946442170029,\n",
              "  0.0005385013079681728,\n",
              "  0.0005579506158444585,\n",
              "  0.0005941123804905222,\n",
              "  0.0006610206075543796,\n",
              "  0.0006793749162540872,\n",
              "  0.000623309393873552,\n",
              "  0.0006272683297519183,\n",
              "  0.0006066242975280177,\n",
              "  0.0006217941279948823,\n",
              "  0.0006136423403570285,\n",
              "  0.000623630203019944,\n",
              "  0.0005698232951699922,\n",
              "  0.0005557133510083684,\n",
              "  0.0005791423441699014,\n",
              "  0.0005719269940210378,\n",
              "  0.0005838368075471977,\n",
              "  0.000592908624974892,\n",
              "  0.0005764791615523102,\n",
              "  0.0006154878399192506,\n",
              "  0.0006575778744329136,\n",
              "  0.0006444051520683163,\n",
              "  0.000657347621465951,\n",
              "  0.0006678389831653416,\n",
              "  0.0006357273349224766,\n",
              "  0.0006289686028284394,\n",
              "  0.0005855623082857906,\n",
              "  0.0008344600080972606,\n",
              "  0.0005835618758244162,\n",
              "  0.0006225800119310994,\n",
              "  0.0008255728841366056,\n",
              "  0.0008425374191619072,\n",
              "  0.0008118875000991197,\n",
              "  0.0008444485506606945,\n",
              "  0.0008394160828498795,\n",
              "  0.0008357162663151414,\n",
              "  0.0006422337448396137,\n",
              "  0.0006322659413828452,\n",
              "  0.0008586269284340723,\n",
              "  0.0008725831149882091,\n",
              "  0.0009106415995598692,\n",
              "  0.0009187903919205645,\n",
              "  0.0009404707611739589,\n",
              "  0.0009238981851649831,\n",
              "  0.0008894173653464791,\n",
              "  0.0008890688388441361,\n",
              "  0.0008684511311220393,\n",
              "  0.0008666577558896139,\n",
              "  0.0008349203538189639,\n",
              "  0.0008164217566302307,\n",
              "  0.0008287220635499784,\n",
              "  0.0007971410353180047,\n",
              "  0.0007872997608002286,\n",
              "  0.0006989878133148471,\n",
              "  0.0007086996362197953,\n",
              "  0.0007464864468610005,\n",
              "  0.0007606497960263392,\n",
              "  0.0007640210651524719,\n",
              "  0.000760636490876195,\n",
              "  0.0008279027905579449,\n",
              "  0.0008672269386157386,\n",
              "  0.0009249533469609443,\n",
              "  0.000987388528554505,\n",
              "  0.0009714819543262383,\n",
              "  0.0009734891733665364,\n",
              "  0.0009737640963019696,\n",
              "  0.0009580700758839746,\n",
              "  0.0009411278326497515,\n",
              "  0.0009682899407315602,\n",
              "  0.0009574288699068697,\n",
              "  0.0009678865348064369,\n",
              "  0.0009501803159717079,\n",
              "  0.0009416833449117569,\n",
              "  0.0009663329140398797,\n",
              "  0.0009756235560960679,\n",
              "  0.0009668189659866277,\n",
              "  0.0009130433541557297,\n",
              "  0.0009108006577169559,\n",
              "  0.0009106499420910785,\n",
              "  0.0009088508058990696,\n",
              "  0.0008841263278025975,\n",
              "  0.0008302556748533967,\n",
              "  0.0008406375081600551,\n",
              "  0.0008292181948548408,\n",
              "  0.0008286412280753767,\n",
              "  0.0008190503147188069,\n",
              "  0.0007925984593249258,\n",
              "  0.000780116330523378,\n",
              "  0.000773593124966489,\n",
              "  0.0007583073705715017,\n",
              "  0.0007648987147020527,\n",
              "  0.000724174167232281,\n",
              "  0.0007533595351632142,\n",
              "  0.0007813525937718636,\n",
              "  0.0007589174094918545,\n",
              "  0.0007552311592481952,\n",
              "  0.0007684031179536954,\n",
              "  0.000744806198737637,\n",
              "  0.0007010801066063011,\n",
              "  0.000699475415066735,\n",
              "  0.0006783673521690621,\n",
              "  0.0006355171488098779,\n",
              "  0.0006207088227427817,\n",
              "  0.0006313217010803444,\n",
              "  0.0006286532160571913,\n",
              "  0.0006350691105021249,\n",
              "  0.0006297910841960246,\n",
              "  0.000622248462966254,\n",
              "  0.0005824204309033932,\n",
              "  0.0005270431942437264,\n",
              "  0.0004995905286971968,\n",
              "  0.0005323840081770984,\n",
              "  0.0005205840784205626,\n",
              "  0.000521663066244464,\n",
              "  0.0005156588005342595,\n",
              "  0.0005346911628443682,\n",
              "  0.0005451639317359359,\n",
              "  0.0005591950142505033,\n",
              "  0.0004933435382070679,\n",
              "  0.0004499421694317967,\n",
              "  0.0004495918819061611,\n",
              "  0.00045581724382825947,\n",
              "  0.00042906424397008155,\n",
              "  0.000452294981243131,\n",
              "  0.00048640938915032913,\n",
              "  0.0004829801316679421,\n",
              "  0.0004922348657435314,\n",
              "  0.0004716209231338796,\n",
              "  0.000458304762974972,\n",
              "  0.00045324726379199475,\n",
              "  0.0004437058531230165,\n",
              "  0.0004143617502532548,\n",
              "  0.0004486240940519493,\n",
              "  0.0004491129983717217,\n",
              "  0.0005458965575212862,\n",
              "  0.0005814848248583662,\n",
              "  0.0007376583933179085,\n",
              "  0.0005973475604201462,\n",
              "  0.0007228135677942345,\n",
              "  0.0007295605210580541,\n",
              "  0.0007269499131328813,\n",
              "  0.000710967052716392,\n",
              "  0.000532610394277698,\n",
              "  0.0005426374153285973,\n",
              "  0.00045865989824685476,\n",
              "  0.0004554282769515985,\n",
              "  0.00045701540893587725,\n",
              "  0.00047530044222548817,\n",
              "  0.00046783301040335745,\n",
              "  0.0004669134221880084,\n",
              "  0.00033037433879136744,\n",
              "  0.00033563497267307226,\n",
              "  0.00033123566312876515,\n",
              "  0.00029662667965592245,\n",
              "  0.00029737586814539504,\n",
              "  0.0002774385404538074,\n",
              "  0.0002857580951317038,\n",
              "  0.0003021067820855777,\n",
              "  0.0003211919654722282,\n",
              "  0.0003590719032605245,\n",
              "  0.0003861486712334696,\n",
              "  0.00035058579509516434,\n",
              "  0.0003459207044447132,\n",
              "  0.00033994384007819195,\n",
              "  0.00030645135693867703,\n",
              "  0.0003296939502171859,\n",
              "  0.0005209653019692296,\n",
              "  0.0005369680327292612,\n",
              "  0.0005627168203387235,\n",
              "  0.0005627036224146091,\n",
              "  0.0005798351302929947,\n",
              "  0.0005706799844733079,\n",
              "  0.0005501368639604526,\n",
              "  0.0005663119723784948,\n",
              "  0.0005105273555562907,\n",
              "  0.0004774312868952055,\n",
              "  0.00046924212233381943,\n",
              "  0.00043431252834098,\n",
              "  0.00043306289784530397,\n",
              "  0.00041777980920517527,\n",
              "  0.00042058610312328056,\n",
              "  0.0004266776227970275,\n",
              "  0.00042733363934949024,\n",
              "  0.0004340641985300377,\n",
              "  0.0004159756733135114,\n",
              "  0.0004322793053471564,\n",
              "  0.00043799754757718317,\n",
              "  0.00044118777954370704,\n",
              "  0.0004498052004907951,\n",
              "  0.00045313458786178237,\n",
              "  0.00047354711996424605,\n",
              "  0.0005688868554706047,\n",
              "  0.000552485032281309,\n",
              "  0.0005555662984466468,\n",
              "  0.0005565806161827403,\n",
              "  0.0007560370339003508,\n",
              "  0.0007506925702179993,\n",
              "  0.0005337365863077939,\n",
              "  0.000522566482424701,\n",
              "  0.0005415122939598292,\n",
              "  0.0005351152215618195,\n",
              "  0.0005272465174116337,\n",
              "  0.0005327211450939969,\n",
              "  0.0005321749948974744,\n",
              "  0.0005186451232879238,\n",
              "  0.0005294060248013894,\n",
              "  0.0005213814238149134,\n",
              "  0.0005056215850343155,\n",
              "  0.0005063690877264729,\n",
              "  0.0005135882583338657,\n",
              "  0.0005196438621677092,\n",
              "  0.0005290150867818733,\n",
              "  0.0008012844150293181,\n",
              "  0.0007954292222911907,\n",
              "  0.0007892009010506926,\n",
              "  0.0009791738051471294,\n",
              "  0.0010081381066156738,\n",
              "  0.000982003376626906,\n",
              "  0.0010296149967315101,\n",
              "  0.0010395328763952417,\n",
              "  0.00102518755241315,\n",
              "  0.001054848999122131,\n",
              "  0.0010595871901560605,\n",
              "  0.0010672295541708583,\n",
              "  0.0010494289540429929,\n",
              "  0.0010525961451749175,\n",
              "  0.0010543546580051417,\n",
              "  0.00125790152646565,\n",
              "  0.0010544421320810421,\n",
              "  0.001274565246895343,\n",
              "  0.0012803243634711798,\n",
              "  0.0012820861765964922,\n",
              "  0.0013129576670966705,\n",
              "  0.0013230784349492154,\n",
              "  0.0013124338559233537,\n",
              "  0.0013280780927138534,\n",
              "  0.0013525914136101925,\n",
              "  0.00134154659691715,\n",
              "  0.0013439852662640434,\n",
              "  0.0013566314786386696,\n",
              "  0.0013629061796209053,\n",
              "  0.0013685155110952731,\n",
              "  0.0013480604668309726,\n",
              "  0.0013725769990327451,\n",
              "  0.0013748416807411949,\n",
              "  0.0013835430860410177,\n",
              "  0.001381344242277526,\n",
              "  0.0013735662451406423,\n",
              "  0.0013762344046540666,\n",
              "  0.0013644405568523693,\n",
              "  0.0013728193422117801,\n",
              "  0.0013578671573168638,\n",
              "  0.0011460201629400633,\n",
              "  0.0011360634014066437,\n",
              "  0.00114103325867641,\n",
              "  0.0011415909539804858,\n",
              "  0.00114578736115815,\n",
              "  0.001140843318428616,\n",
              "  0.001155720977417498,\n",
              "  0.0011635234971051883,\n",
              "  0.0011716897945515611,\n",
              "  0.0011389125224509039,\n",
              "  0.0011499269742863574,\n",
              "  0.0010989216906023911,\n",
              "  0.0010845008899065863]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqfTbIs9-1G7",
        "colab_type": "code",
        "outputId": "a495f5c7-bf2e-43cc-cd50-b479dc11b8cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!cat >test.sh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pwd\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSwSqDu2GJlv",
        "colab_type": "code",
        "outputId": "eced1c1c-ef22-4c1f-a374-a5ada7591b0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!bash test.sh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/LightGBM/python-package\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P80RnGE8GZX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}