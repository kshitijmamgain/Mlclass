{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_class_demonstration.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOgb7S8gqSjoOkiLjjRTcci",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kshitijmamgain/Mlclass/blob/master/ML_class_demonstration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFnO7FWiMbZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install optuna"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el7srxVor8sT",
        "colab_type": "text"
      },
      "source": [
        "# Getting Started\n",
        "This notebook is for the new and aspirant data scientists to demonstrate the use of object oriented programming in Python to tune the machine learning (ML) models .\n",
        "The notebook is divided into 2 parts so suit yourself to jump to the section you feel is appropriate for you.\n",
        "1. Introduction to Bayesian Optimization\n",
        "2. ML Object Classes for hyper-parameter tuning\n",
        "\n",
        "We would use breast cancer dataset in sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QJa44lFUIHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Apr 14 16:11:15 2020\n",
        "\n",
        "@author: Kshitij\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# use available dataset for classification\n",
        "dataset = load_breast_cancer()\n",
        "\n",
        "X = dataset.data\n",
        "y = dataset.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 2/3, random_state = 1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CYYPTPk_xBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' We will be using lighgbm api without sklearn so we will provide the params\n",
        " in a dict format'''\n",
        "param = param = {'objective': 'binary', 'learning_rate': 0.5,\n",
        "                 'reg_alpha': 0.5, 'reg_lambda': 0.5}\n",
        "\n",
        "param['metric'] = 'auc'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QEMK-BkAA7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model\n",
        "model = lgb.train(param, lgb.Dataset(X_train, label = y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "343jQMB2ATQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make prediction on test dataset\n",
        "# unlike sklearn classifier the 'predict' method gives probability in lightgbm\n",
        "pred=model.predict(X_test)\n",
        "# we get the y_pred with threshold at 0.5\n",
        "y_pred = np.where(pred>0.5,1,0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyeMBaPFA-6D",
        "colab_type": "code",
        "outputId": "f976d3b9-0b68-42c6-ad7a-147a0633752f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
              "       0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "       1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
              "       0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
              "       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hRYoGnr8daH",
        "colab_type": "code",
        "outputId": "aaba1b72-196b-46c2-9939-70dcf68a4156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# see the f1 score: harmonic mean of precision and recall\n",
        "f1_score(y_test, y_pred)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9561752988047808"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWgRB59uvxyG",
        "colab_type": "text"
      },
      "source": [
        "<b>Conclusion:</b> The above f1-score is obtained by selecting some random value for the hyper-parameters but this could be improved using bayesian optimization for hyper-parameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K0oS7dPt50U",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Bayesian Optimization\n",
        "Not too long ago selection of hyper-parameters was termed as the 'art', some would say it requires skills to tune while others would say it's based on intuition. Additionally, the more complex the ML algorithm gets the more are the hyper-parameters to tune.\n",
        "\n",
        "Fortunately, there are better ways to tune the hyper-parameters which are more effective than a random search like [Bayesian Optimization](https://arxiv.org/pdf/1807.02811.pdf), which arrives on the best hyper-parameter values by estimating and updating the probability distribution that describes potential values of the objective function. In simple word this means when the algorithm finds a good result for particular value of a hyper-parameter it intensifies the search around that value. We would discuss two python libraries __Hyperopt__ and __Optuna__ which use such approach.\n",
        "1. Hyperopt Tuning<br>\n",
        "In the codes below five methods have been called from Hypeopt library - <b>fmin</b>, <b>hp</b>, <b>tpe</b>, <b>trials</b> and <b>STATUS_OK</b>. <br>The first, <b>fmin</b> is the method that minimizes the objective function loss. If we want our model to perform with higher accuracy then what we would like to minimize is (1 minus accuracy) in same way if we want to minimize the F1score, the loss that we would minimize is (1 minus F1score).<br>\n",
        "Next we need a parameter space from where the value of hyper-parameters would be selected. Such search space is defined by <b>hp</b><br>\n",
        "The <b>tpe</b> method has algorithm to search from the space defined above and <b>Trials</b> method creates a database to to record the trials. And finally <b>STATUS_OK</b> which is mandatory to be returned from objective function to store the success of the run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7JBKwOiHm4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from hyperopt import  fmin, hp, tpe, Trials, STATUS_OK\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lklswk7NL3E2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def objective(params):\n",
        "    \n",
        "    h_model = lgb.train(params, lgb.Dataset(X_train, label = y_train))\n",
        "    pred=h_model.predict(X_test)\n",
        "    y_pred = np.array(list(map(lambda x: int(x), pred>0.5)))\n",
        "    f1sc = f1_score(y_test, y_pred)\n",
        "    loss = 1 - f1sc\n",
        "    return {'loss': loss, 'status' : STATUS_OK}\n",
        "\n",
        "space = {\n",
        "    'lambda_l1': hp.uniform('lambda_l1', 0.0, 1.0),\n",
        "    'lambda_l2': hp.uniform(\"lambda_l2\", 0.0, 1.0),\n",
        "    'learning_rate' : hp.loguniform('learning_rate', np.log(0.05), np.log(0.25)),\n",
        "    'objective' : 'binary',\n",
        "    'metric' : 'auc'\n",
        "    }\n",
        "trials = Trials()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwZt-kSP1ICg",
        "colab_type": "code",
        "outputId": "d94219ef-c0e9-42a3-d943-06a482abab81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# deine the maximum evaluations\n",
        "best = fmin(fn=objective, space=space, algo=tpe.suggest, trials= trials, max_evals=100)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:09<00:00, 10.39it/s, best loss: 0.02788844621513953]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KYgvtADyyMe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "df37d227-6bd2-416d-bfcd-0dc5731eceea"
      },
      "source": [
        "best"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lambda_l1': 0.16001564250154893,\n",
              " 'lambda_l2': 0.19419820383038114,\n",
              " 'learning_rate': 0.13580995267128684}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxMHklxo2p1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model on best parameter results\n",
        "h_model = lgb.train(best, lgb.Dataset(X_train, label = y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q7NIuyrFzO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the y_pred\n",
        "pred_h=h_model.predict(X_test)\n",
        "\n",
        "y_predh = list(map(lambda x: int(x), pred_h>0.5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XBtkMQcGMtw",
        "colab_type": "code",
        "outputId": "bda8d7e9-9eb5-4164-8f28-31599968c15b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "f1_score(y_test, y_predh)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9682539682539683"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcbb9MlyzFM3",
        "colab_type": "text"
      },
      "source": [
        "<b>Conclusion:</b> We see that the new optimized parameters obtained have improved the f1-score on test dataset.<br>\n",
        "Also notice that when fmin method is called we also have to specify the number of maximum evaluation or the number of time the space would be searched to predict the optimum parameters.\n",
        "\n",
        "2. Optuna<br>\n",
        "Optuna library too optimizes the hyper-parameters with Bayesian Optimization. But there is ease in coding over Hyperopt, firstly, we define objective function and hyper-parameter space inside a single function. Secondly, unlike Hyperopt which only 'minimizes' the objective function in Optuna we could define if we wish to maximize or minimize the objective.<br>\n",
        "\n",
        "Notice that we have used 'minimize' as direction since we want an output similar to Hyperopt. We could have also returned f1-score with direction as 'maximum' in Optuna. The optimized hyper-parameters are stored in best_params attribute in study."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKWXLT18Vw2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import optuna\n",
        "import optuna.integration.lightgbm as lgbo\n",
        "def optuna_obj(trial):\n",
        "    '''Defining the parameters space inside the function for optuna optimization'''\n",
        "    params = {\n",
        "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
        "        'lambda_l2': trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
        "        'learning_rate' : trial.suggest_loguniform('learning_rate', 0.05, 0.25),\n",
        "        'objective' : 'binary',\n",
        "        'metric' : 'auc'\n",
        "            }\n",
        "    o_model = lgb.train(params, lgbo.Dataset(X_train, label = y_train))\n",
        "    pred=o_model.predict(X_test)\n",
        "    y_pred = np.array(list(map(lambda x: int(x), pred>0.5)))\n",
        "    f1sc = f1_score(y_test, y_pred)\n",
        "    loss = 1 - f1sc\n",
        "    return loss\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(optuna_obj, n_trials=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWhDET9hW8_M",
        "colab_type": "code",
        "outputId": "60a46404-6ef3-4193-a3f1-4c2323e7a377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "study.best_params"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lambda_l1': 5.555188524228337e-05,\n",
              " 'lambda_l2': 6.246148576377471e-06,\n",
              " 'learning_rate': 0.13212756681506424}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I9LXyqJhU_T",
        "colab_type": "code",
        "outputId": "ebdee1cd-aafd-45ff-c3a5-c070c4c24ac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "o_model = lgb.train(study.best_params, lgb.Dataset(X_train, label = y_train))\n",
        "pred_o=o_model.predict(X_test)\n",
        "\n",
        "y_predo = list(map(lambda x: int(x), pred_o>0.5))\n",
        "f1_score(y_test, y_predo)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9603174603174603"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfGbloLT0QUF",
        "colab_type": "text"
      },
      "source": [
        "__Conclusion__: We see that using Optuna to improved the results from the earlier result<br> We should also observe that our initial parameters and the best parameters from Hyperopt and Optuna are all different. While it is understandable for manually defined parameters to be different, the reason for different Optuna and Hyperopt best parameters could lie with maximum evaluation and the internal algorithm's approach in optimizing. The f1-score does improve with the new parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeULzN-j1FU-",
        "colab_type": "text"
      },
      "source": [
        "# ML Object Classes for hyper-parameter tuning\n",
        "The focus in this section would be to demonstrate how to create a classifier class. There are plenty of resources available on OOP programming but demonstration of complete classes is limited. We would use the above example to create a class. In building a class it is important to visualize the structure of your class, here we would create a simple flow as shown in the figure below.\n",
        "![alt text](https://raw.githubusercontent.com/kshitijmamgain/Mlclass/master/Classflow.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPEnqgSFhlzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mlclass():\n",
        "    '''Parameter Tuning Class tunes the LightGBM model with different   optimization techniques - Hyperopt, Optuna.'''\n",
        "    def __init__(self, x_train, y_train):\n",
        "        '''Initializes the Parameter tuning class and also initializes   LightGBM dataset object\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_train: data (string, numpy array, pandas DataFrame,or list of numpy arrays) – Data source of Dataset.\n",
        "        y_train: label (list, numpy 1-D array, pandas Series / one-column DataFrame or None – Label of the data.'''\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.train_set = lgb.Dataset(data=x_train, label=y_train)\n",
        "\n",
        "    def tuning(self, optim_type):\n",
        "        '''Method takes the optimization type and tunes the model'''\n",
        "        #call the optim_type: Hyperopt or Optuna\n",
        "        optimization = getattr(self, optim_type)\n",
        "        return optimization()\n",
        "  \n",
        "    def hyperopt_method(self):\n",
        "        # This method is called by tuning when user inputs 'hyperopt_method' while calling the tuning method\n",
        "    \n",
        "        #define the hyperopt space\n",
        "        space = {'lambda_l1': hp.uniform('lambda_l1', 0.0, 1.0),\n",
        "                 'lambda_l2': hp.uniform(\"lambda_l2\", 0.0, 1.0),\n",
        "                 'learning_rate' : hp.loguniform('learning_rate',\n",
        "                                                 np.log(0.05), np.log(0.25)),\n",
        "                 'objective' : 'binary'}\n",
        "        # define algorithm and trials inside the class\n",
        "        algo, trials= tpe.suggest, Trials()\n",
        "        #Call the fmin from inside the class\n",
        "        best = fmin(fn=objective,space=space,algo=algo,trials=trials,max_evals=1000)\n",
        "        self.params = best\n",
        "        return best, trials\n",
        "    def objective(self, params):\n",
        "        # same objective function with added self\n",
        "        h_model = lgb.train(params, lgb.Dataset(X_train, label = y_train))\n",
        "        pred=h_model.predict(X_test)\n",
        "        y_pred = np.array(list(map(lambda x: int(x), pred>0.5)))\n",
        "        f1sc = f1_score(y_test, y_pred)\n",
        "        loss = 1 - f1sc\n",
        "        return {'loss': loss,'status' : STATUS_OK}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuHjOLFX2pRU",
        "colab_type": "text"
      },
      "source": [
        "The ML class method is first initialized with training dataset and the target. A class is defined by starting with class. The functions defined inside the class are known as methods. Our first method is __init__ which is used to initialize the class. Here we want our class to be initialized with data-set and the target so we have given the inputs parameters as 'x_train' and 'y_train'. The self in the method is used to associate the function with an instance. Using 'self. ' as prefix to the variables also makes the class variables specific to that instance. Calling this class is as easy as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARPGyMZWX1Sm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Obj = Mlclass(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdHUxaYD3YHz",
        "colab_type": "text"
      },
      "source": [
        "Once the class method is initialized we would add the method for Hypeorpt optimization. We would want user to input optimization type as Hypeorpt and then tune the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTadaMcybcCs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7cd3b9c3-0539-40a7-e86e-9755cf2598a9"
      },
      "source": [
        "Obj.tuning('hyperopt_method')"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [01:47<00:00,  9.29it/s, best loss: 0.019920318725099473]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'lambda_l1': 0.00013088713969735405,\n",
              "  'lambda_l2': 0.0007421261199961893,\n",
              "  'learning_rate': 0.12828502142851694},\n",
              " <hyperopt.base.Trials at 0x7f21bdfc9da0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2ucpeUV3lkg",
        "colab_type": "text"
      },
      "source": [
        "Let's add a similar method for Optuna"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDsGbkvQdyGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mlclass():\n",
        "    '''Parameter Tuning Class tunes the LightGBM model with different   optimization techniques - Hyperopt, Optuna.'''\n",
        "    def __init__(self, x_train, y_train):\n",
        "        '''Initializes the Parameter tuning class and also initializes   LightGBM dataset object\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_train: data (string, numpy array, pandas DataFrame,or list of numpy arrays) – Data source of Dataset.\n",
        "        y_train: label (list, numpy 1-D array, pandas Series / one-column DataFrame or None – Label of the data.'''\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.train_set = lgb.Dataset(data=x_train, label=y_train)\n",
        "\n",
        "    def tuning(self, optim_type):\n",
        "        '''Method takes the optimization type and tunes the model'''\n",
        "        #call the optim_type: Hyperopt or Optuna\n",
        "        optimization = getattr(self, optim_type)\n",
        "        return optimization()\n",
        "     \n",
        "    def optuna_method(self):\n",
        "        study = optuna.create_study(direction='minimize')\n",
        "        study.optimize(optuna_obj, n_trials=1000)\n",
        "        self.params = study.best_params\n",
        "        return study\n",
        "    \n",
        "    def optuna_obj(self, trial):\n",
        "        '''Same optuna objective with parameters space inside the function for optuna optimization'''\n",
        "        params = {'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
        "                  'lambda_l2': trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
        "                  'learning_rate' : trial.suggest_loguniform('learning_rate', 0.05, 0.25)}\n",
        "        \n",
        "        o_model = lgb.train(params, lgbo.Dataset(X_train, label = y_train))\n",
        "        pred=o_model.predict(X_test)\n",
        "        y_pred = np.array(list(map(lambda x: int(x), pred>0.5)))\n",
        "        f1sc = f1_score(y_test, y_pred)\n",
        "        loss = 1 - f1sc\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0bMNdJ7ggZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Obj = Mlclass(X_train, y_train)\n",
        "Obj.tuning('optuna_method')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyRPnder38-x",
        "colab_type": "text"
      },
      "source": [
        "To use the best parameters for evaluation we defined another variable _self.params_ which would be defined for that instance and could be accessed by a yet to be defined train method.<br>\n",
        "Once the model is trained we could evaluate that by giving test data-set and test target as the parameters. All the combined methods of the class are shown below\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFWnrcREgnH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mlclass():\n",
        "    '''Parameter Tuning Class tunes the LightGBM model with different   optimization techniques - Hyperopt, Optuna.'''\n",
        "    def __init__(self, x_train, y_train):\n",
        "        '''Initializes the Parameter tuning class and also initializes   LightGBM dataset object\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_train: data (string, numpy array, pandas DataFrame,or list of numpy arrays) – Data source of Dataset.\n",
        "        y_train: label (list, numpy 1-D array, pandas Series / one-column DataFrame or None – Label of the data.'''\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.train_set = lgb.Dataset(data=x_train, label=y_train)\n",
        "\n",
        "    def tuning(self, optim_type):\n",
        "        '''Method takes the optimization type and tunes the model'''\n",
        "        #call the optim_type: Hyperopt or Optuna\n",
        "        optimization = getattr(self, optim_type)\n",
        "        return optimization()\n",
        "  \n",
        "    def hyperopt_method(self):\n",
        "        # This method is called by tuning when user inputs 'hyperopt_method' while calling the tuning method\n",
        "    \n",
        "        #define the hyperopt space\n",
        "        space = {'lambda_l1': hp.uniform('lambda_l1', 0.0, 1.0),\n",
        "                 'lambda_l2': hp.uniform(\"lambda_l2\", 0.0, 1.0),\n",
        "                 'learning_rate' : hp.loguniform('learning_rate',\n",
        "                                                 np.log(0.05), np.log(0.25)),\n",
        "                 'objective' : 'binary'}\n",
        "        # define algorithm and trials inside the class\n",
        "        algo, trials= tpe.suggest, Trials()\n",
        "        #Call the fmin from inside the class\n",
        "        best = fmin(fn=objective,space=space,algo=algo,trials=trials,max_evals=1000)\n",
        "        self.params = best\n",
        "        return best, trials\n",
        "    def objective(self, params):\n",
        "        # same objective function with added self\n",
        "        h_model = lgb.train(params, lgb.Dataset(X_train, label = y_train))\n",
        "        pred=h_model.predict(X_test)\n",
        "        y_pred = np.array(list(map(lambda x: int(x), pred>0.5)))\n",
        "        f1sc = f1_score(y_test, y_pred)\n",
        "        loss = 1 - f1sc\n",
        "        return {'loss': loss,'status' : STATUS_OK}\n",
        "    \n",
        "    def optuna_method(self):\n",
        "        study = optuna.create_study(direction='minimize')\n",
        "        study.optimize(optuna_obj, n_trials=1000)\n",
        "        self.params = study.best_params\n",
        "        return study\n",
        "    \n",
        "    def optuna_obj(self, trial):\n",
        "        '''Same optuna objective with parameters space inside the function for optuna optimization'''\n",
        "        params = {'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
        "                  'lambda_l2': trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
        "                  'learning_rate' : trial.suggest_loguniform('learning_rate', 0.05, 0.25)}\n",
        "        \n",
        "        o_model = lgb.train(params, lgbo.Dataset(X_train, label = y_train))\n",
        "        pred=o_model.predict(X_test)\n",
        "        y_pred = np.array(list(map(lambda x: int(x), pred>0.5)))\n",
        "        f1sc = f1_score(y_test, y_pred)\n",
        "        loss = 1 - f1sc\n",
        "        return loss\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"This function evaluates the model on best parameters\"\"\"\n",
        "        print(\"Model will be trained on the following parameters: \\n{}\".format(self.params))\n",
        "        #train the model with best parameters\n",
        "        self.gbm = lgb.train(self.params, self.train_set)\n",
        "    def evaluate(self, x_test, y_test):\n",
        "        # predict the values from x_test\n",
        "        pred = self.gbm.predict(x_test)\n",
        "        y_pred = np.where(pred>0.5,1,0)\n",
        "        #print confusion matrix\n",
        "        print(confusion_matrix(y_test,y_pred))\n",
        "        #print classification report\n",
        "        print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlIIqKh0oZoc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "229313b5-9da2-43e6-f458-0e754dc2025e"
      },
      "source": [
        "Obj = Mlclass(X_train, y_train)\n",
        "Obj.tuning('hyperopt_method')\n",
        "Obj.train()\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [01:46<00:00,  9.38it/s, best loss: 0.02400000000000002]\n",
            "Model will be trained on the following parameters: \n",
            "{'lambda_l1': 0.004494621171141966, 'lambda_l2': 0.014337729938273924, 'learning_rate': 0.09527473901625513}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-4ebcc22b63a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hyperopt_method'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: evaluate() missing 2 required positional arguments: 'x_test' and 'y_test'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULQiVYtvot-K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2ff692d1-6659-40c6-fe8a-561451d3dd2e"
      },
      "source": [
        "Obj.evaluate(X_test, y_test)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 60   6]\n",
            " [  2 122]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94        66\n",
            "           1       0.95      0.98      0.97       124\n",
            "\n",
            "    accuracy                           0.96       190\n",
            "   macro avg       0.96      0.95      0.95       190\n",
            "weighted avg       0.96      0.96      0.96       190\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5vo6IbEpWtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}